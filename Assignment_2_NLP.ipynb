{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 330.3 kB/s eta 0:00:39\n",
      "     --------------------------------------- 0.0/12.8 MB 281.8 kB/s eta 0:00:46\n",
      "     --------------------------------------- 0.1/12.8 MB 595.3 kB/s eta 0:00:22\n",
      "      -------------------------------------- 0.2/12.8 MB 787.7 kB/s eta 0:00:17\n",
      "      -------------------------------------- 0.3/12.8 MB 947.5 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.7/12.8 MB 1.7 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 0.9/12.8 MB 1.9 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.2/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.3/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.7/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.0/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.1/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.3/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.5/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.6/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.7/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.9/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 2.9/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.0/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.4 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.0/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.1/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.3/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.4/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.6/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.7/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.9/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.8/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.1/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.2/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.4/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.7/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.9/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 7.0/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 8.0/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.3/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.6/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.8/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.6/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.9/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 3.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.5/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.4)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.17.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install pandas\n",
    "!pip install -U pip setuptools wheel\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install openpyxl\n",
    "!pip install scikit-learn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import sparse\n",
    "from sklearn.utils.extmath import randomized_svd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing the text**\n",
    "The steps will be: \n",
    "* Tokenization\n",
    "* Lemmatization\n",
    "* Cleaning the Lemmatized tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Functions (using spaCy) for Tokenization, Lemmatization & Cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def lemmatize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "def clean_text_spacy(tokens):\n",
    "    cleaned_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will be using a Whatsapp conversation as our corpus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_chat.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "data = []\n",
    "for row in content.split('\\n'):\n",
    "    data.append(row[row.rfind(':') + 1:])\n",
    "\n",
    "whatsapp_df = pd.DataFrame({\"whatsapp_text\":data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Applying Tokenization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsapp_df['scraped_tokens'] = whatsapp_df['whatsapp_text'].apply(tokenize_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Applying Lemmatization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsapp_df['scraped_lemmatization'] = whatsapp_df['whatsapp_text'].apply(lemmatize_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cleaning the lemmatized tokens*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsapp_df['cleaned_tokens'] = whatsapp_df['scraped_lemmatization'].apply(clean_text_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whatsapp_df_copy = whatsapp_df.copy()\n",
    "# whatsapp_df_copy['scraped_tokens'] = whatsapp_df_copy['scraped_tokens'].apply(lambda x: ' '.join(x))\n",
    "# whatsapp_df_copy['scraped_lemmatization'] = whatsapp_df_copy['scraped_lemmatization'].apply(lambda x: ' '.join(x))\n",
    "# whatsapp_df_copy['cleaned_tokens'] = whatsapp_df_copy['cleaned_tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsapp_df['filtered_tokens'] = whatsapp_df['cleaned_tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>whatsapp_text</th>\n",
       "      <th>scraped_tokens</th>\n",
       "      <th>scraped_lemmatization</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚ÄéMessages and calls are end-to-end encrypted....</td>\n",
       "      <td>[ , ‚ÄéMessages, and, calls, are, end, -, to, -,...</td>\n",
       "      <td>[ , ‚Äémessage, and, call, be, end, -, to, -, en...</td>\n",
       "      <td>[and, call, be, end, to, end, encrypt, no, one...</td>\n",
       "      <td>end end encrypt outside chat whatsapp read listen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚ÄéYou created group ‚Äú◊û◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊†◊í◊ú◊ô◊™‚Äù</td>\n",
       "      <td>[ , ‚ÄéYou, created, group, ‚Äú, ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™, ‚Äù]</td>\n",
       "      <td>[ , ‚Äéyou, create, group, \", ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™, \"]</td>\n",
       "      <td>[create, group, ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™]</td>\n",
       "      <td>create group ◊û◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊†◊í◊ú◊ô◊™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey noam how are you?</td>\n",
       "      <td>[ , hey, noam, how, are, you, ?]</td>\n",
       "      <td>[ , hey, noam, how, be, you, ?]</td>\n",
       "      <td>[hey, noam, how, be, you]</td>\n",
       "      <td>hey noam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey!! I‚Äôm good thanks how about you</td>\n",
       "      <td>[ , Hey, !, !, I, ‚Äôm, good, thanks, how, about...</td>\n",
       "      <td>[ , Hey, !, !, I, ‚Äôm, good, thank, how, about,...</td>\n",
       "      <td>[hey, i, good, thank, how, about, you]</td>\n",
       "      <td>hey good thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are you going to do today</td>\n",
       "      <td>[ , What, are, you, going, to, do, today]</td>\n",
       "      <td>[ , what, be, you, go, to, do, today]</td>\n",
       "      <td>[what, be, you, go, to, do, today]</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       whatsapp_text  \\\n",
       "0   ‚ÄéMessages and calls are end-to-end encrypted....   \n",
       "1                ‚ÄéYou created group ‚Äú◊û◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊†◊í◊ú◊ô◊™‚Äù   \n",
       "2                              hey noam how are you?   \n",
       "3                Hey!! I‚Äôm good thanks how about you   \n",
       "4                     What are you going to do today   \n",
       "\n",
       "                                      scraped_tokens  \\\n",
       "0  [ , ‚ÄéMessages, and, calls, are, end, -, to, -,...   \n",
       "1   [ , ‚ÄéYou, created, group, ‚Äú, ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™, ‚Äù]   \n",
       "2                   [ , hey, noam, how, are, you, ?]   \n",
       "3  [ , Hey, !, !, I, ‚Äôm, good, thanks, how, about...   \n",
       "4          [ , What, are, you, going, to, do, today]   \n",
       "\n",
       "                               scraped_lemmatization  \\\n",
       "0  [ , ‚Äémessage, and, call, be, end, -, to, -, en...   \n",
       "1    [ , ‚Äéyou, create, group, \", ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™, \"]   \n",
       "2                    [ , hey, noam, how, be, you, ?]   \n",
       "3  [ , Hey, !, !, I, ‚Äôm, good, thank, how, about,...   \n",
       "4              [ , what, be, you, go, to, do, today]   \n",
       "\n",
       "                                      cleaned_tokens  \\\n",
       "0  [and, call, be, end, to, end, encrypt, no, one...   \n",
       "1                   [create, group, ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™]   \n",
       "2                          [hey, noam, how, be, you]   \n",
       "3             [hey, i, good, thank, how, about, you]   \n",
       "4                 [what, be, you, go, to, do, today]   \n",
       "\n",
       "                                     filtered_tokens  \n",
       "0  end end encrypt outside chat whatsapp read listen  \n",
       "1                        create group ◊û◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊†◊í◊ú◊ô◊™  \n",
       "2                                           hey noam  \n",
       "3                                     hey good thank  \n",
       "4                                              today  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatsapp_df_copy = whatsapp_df.copy()\n",
    "whatsapp_df_copy['filtered_tokens'] = whatsapp_df_copy['filtered_tokens'].apply(lambda x: ' '.join(x))\n",
    "whatsapp_df_copy.to_excel('whatsapp_df.xlsx')\n",
    "whatsapp_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying Feature Extraction by the following algorithms:**\n",
    "* BOW\n",
    "* TF-IDF\n",
    "* Word embedding by WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bow algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_extraction(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X.toarray(), vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Features:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Feature Names:\n",
      " ['actually' 'add' 'afeka' 'ai' 'amazing' 'analysis' 'anytime' 'article'\n",
      " 'ask' 'awesome' 'backend' 'bad' 'balance' 'balcony' 'basil' 'beautiful'\n",
      " 'believe' 'big' 'bit' 'board' 'book' 'bump' 'bunch' 'busy' 'cake' 'care'\n",
      " 'catch' 'challenge' 'change' 'character' 'chat' 'check' 'coffee' 'come'\n",
      " 'conversation' 'cooking' 'cool' 'couple' 'course' 'create' 'curious'\n",
      " 'currently' 'day' 'debug' 'deck' 'deep' 'definitely' 'development'\n",
      " 'difference' 'dive' 'drag' 'easy' 'eat' 'edit' 'emma' 'encrypt' 'end'\n",
      " 'enhance' 'excited' 'exercise' 'exist' 'exit' 'fantastic' 'far'\n",
      " 'fascinating' 'finally' 'finish' 'forget' 'forward' 'fresh' 'fun' 'game'\n",
      " 'gang' 'garden' 'gardening' 'generate' 'good' 'got' 'gpt' 'great'\n",
      " 'gripping' 'group' 'grow' 'haha' 'hand' 'hangover' 'harari' 'hear' 'herb'\n",
      " 'hey' 'highly' 'history' 'hit' 'hobby' 'hope' 'humankind' 'idea'\n",
      " 'intense' 'interesting' 'job' 'kid' 'kind' 'know' 'language' 'late'\n",
      " 'lately' 'learn' 'let' 'like' 'list' 'listen' 'little' 'look' 'lot'\n",
      " 'love' 'mandalorian' 'maybe' 'mean' 'meet' 'message' 'mint' 'miss'\n",
      " 'natural' 'new' 'nice' 'night' 'nightmare' 'noah' 'noam' 'nostalgia'\n",
      " 'office' 'oh' 'old' 'online' 'outside' 'paragraph' 'perfect'\n",
      " 'photography' 'pick' 'place' 'plan' 'plus' 'preppe' 'problem'\n",
      " 'processing' 'project' 'provoking' 'read' 'rec' 'recipe' 'recommend'\n",
      " 'recommendation' 'relax' 'remember' 'requirement' 'rewarding' 'right'\n",
      " 'role' 'rosemary' 'sam' 'sapiens' 'school' 'setup' 'sign' 'skill' 'sleep'\n",
      " 'software' 'soon' 'sound' 'speak' 'spoon' 'squeeze' 'star' 'start' 'stop'\n",
      " 'storyline' 'stranger' 'struggle' 'stumble' 'super' 'sure' 'surprisingly'\n",
      " 'suspense' 'swamp' 'system' 'talk' 'tell' 'testing' 'text' 'thank'\n",
      " 'therapeutic' 'thing' 'things' 'think' 'tip' 'today' 'touch' 'tough'\n",
      " 'travel' 'try' 'upgrade' 'use' 'want' 'wars' 'watch' 'way' 'weekend'\n",
      " 'whatsapp' 'work' 'write' 'yeah' 'yuval' '◊ë◊ê◊†◊í◊ú◊ô◊™' '◊û◊ì◊ë◊®◊ô◊ù']\n"
     ]
    }
   ],
   "source": [
    "bow_features, bow_feature_names = bow_extraction(whatsapp_df_copy['filtered_tokens'])\n",
    "print(\"BoW Features:\\n\", bow_features)\n",
    "print(\"Feature Names:\\n\", bow_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_extraction(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X.toarray(), vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Features:\n",
      " [[0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.5 0.5]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]]\n",
      "Feature Names:\n",
      " ['actually' 'add' 'afeka' 'ai' 'amazing' 'analysis' 'anytime' 'article'\n",
      " 'ask' 'awesome' 'backend' 'bad' 'balance' 'balcony' 'basil' 'beautiful'\n",
      " 'believe' 'big' 'bit' 'board' 'book' 'bump' 'bunch' 'busy' 'cake' 'care'\n",
      " 'catch' 'challenge' 'change' 'character' 'chat' 'check' 'coffee' 'come'\n",
      " 'conversation' 'cooking' 'cool' 'couple' 'course' 'create' 'curious'\n",
      " 'currently' 'day' 'debug' 'deck' 'deep' 'definitely' 'development'\n",
      " 'difference' 'dive' 'drag' 'easy' 'eat' 'edit' 'emma' 'encrypt' 'end'\n",
      " 'enhance' 'excited' 'exercise' 'exist' 'exit' 'fantastic' 'far'\n",
      " 'fascinating' 'finally' 'finish' 'forget' 'forward' 'fresh' 'fun' 'game'\n",
      " 'gang' 'garden' 'gardening' 'generate' 'good' 'got' 'gpt' 'great'\n",
      " 'gripping' 'group' 'grow' 'haha' 'hand' 'hangover' 'harari' 'hear' 'herb'\n",
      " 'hey' 'highly' 'history' 'hit' 'hobby' 'hope' 'humankind' 'idea'\n",
      " 'intense' 'interesting' 'job' 'kid' 'kind' 'know' 'language' 'late'\n",
      " 'lately' 'learn' 'let' 'like' 'list' 'listen' 'little' 'look' 'lot'\n",
      " 'love' 'mandalorian' 'maybe' 'mean' 'meet' 'message' 'mint' 'miss'\n",
      " 'natural' 'new' 'nice' 'night' 'nightmare' 'noah' 'noam' 'nostalgia'\n",
      " 'office' 'oh' 'old' 'online' 'outside' 'paragraph' 'perfect'\n",
      " 'photography' 'pick' 'place' 'plan' 'plus' 'preppe' 'problem'\n",
      " 'processing' 'project' 'provoking' 'read' 'rec' 'recipe' 'recommend'\n",
      " 'recommendation' 'relax' 'remember' 'requirement' 'rewarding' 'right'\n",
      " 'role' 'rosemary' 'sam' 'sapiens' 'school' 'setup' 'sign' 'skill' 'sleep'\n",
      " 'software' 'soon' 'sound' 'speak' 'spoon' 'squeeze' 'star' 'start' 'stop'\n",
      " 'storyline' 'stranger' 'struggle' 'stumble' 'super' 'sure' 'surprisingly'\n",
      " 'suspense' 'swamp' 'system' 'talk' 'tell' 'testing' 'text' 'thank'\n",
      " 'therapeutic' 'thing' 'things' 'think' 'tip' 'today' 'touch' 'tough'\n",
      " 'travel' 'try' 'upgrade' 'use' 'want' 'wars' 'watch' 'way' 'weekend'\n",
      " 'whatsapp' 'work' 'write' 'yeah' 'yuval' '◊ë◊ê◊†◊í◊ú◊ô◊™' '◊û◊ì◊ë◊®◊ô◊ù']\n"
     ]
    }
   ],
   "source": [
    "tfidf_features, tfidf_feature_names = tfidf_extraction(whatsapp_df_copy['filtered_tokens'])\n",
    "print(\"TF-IDF Features:\\n\", tfidf_features)\n",
    "print(\"Feature Names:\\n\", tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embeddings by Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_extraction(tokens_list, vector_size=100, window=5, min_count=1, workers=4):\n",
    "    model = Word2Vec(sentences=tokens_list, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nice', 0.27789372205734253), ('anytime', 0.24751438200473785), ('exercise', 0.2450217306613922), ('upgrade', 0.2410586029291153), ('things', 0.23789504170417786), ('busy', 0.23460040986537933), ('star', 0.22931070625782013), ('backend', 0.2289053350687027), ('excited', 0.2271011471748352), ('little', 0.19759514927864075)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [text.split() for text in whatsapp_df_copy['filtered_tokens']]\n",
    "\n",
    "word2vec_model = word2vec_extraction(tokenized_texts)\n",
    "similar_words = word2vec_model.wv.most_similar('finish')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Glove Explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations (embeddings) for words. These embeddings capture semantic relationships between words based on their co-occurrence statistics in a large corpus of text. Unlike Word2Vec, which learns embeddings by predicting context words given a target word (skip-gram model) or predicting a target word given context words (continuous bag of words model), GloVe learns embeddings by factorizing the word co-occurrence matrix.\n",
    "\n",
    "Explanation of GloVe:\n",
    "* Co-occurrence Matrix: GloVe starts with a word-word co-occurrence matrix where each element \n",
    "ùëãùëñùëó ‚Äãrepresents how often word ùëó appears in the context of word ùëñ within a fixed window size.\n",
    "\n",
    "* Objective: The goal of GloVe is to learn word embeddings such that the dot product of two word vectors corresponds to the logarithm of their co-occurrence probability.\n",
    "\n",
    "* Training: GloVe uses stochastic gradient descent to minimize a loss function that measures the discrepancy between the dot product of word vectors and the logarithm of their co-occurrence probabilities.\n",
    "\n",
    "* Advantages: GloVe embeddings tend to capture global semantic meanings better than some other models, and they often perform well in tasks requiring understanding of word relationships and analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def build_cooccurrence_matrix(corpus, window_size=2):\n",
    "    vocab = list(set(corpus))\n",
    "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    cooccurrence = lil_matrix((len(vocab), len(vocab)), dtype=np.float64)\n",
    "    \n",
    "    for i, word in enumerate(corpus):\n",
    "        left_context = max(0, i - window_size)\n",
    "        right_context = min(len(corpus), i + window_size + 1)\n",
    "        for j in range(left_context, right_context):\n",
    "            if i != j:\n",
    "                cooccurrence[word_to_id[word], word_to_id[corpus[j]]] += 1\n",
    "    \n",
    "    return cooccurrence.tocsr(), word_to_id\n",
    "\n",
    "def glove_loss(X, W, U, b, c):\n",
    "    diff = W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1)\n",
    "    squared_error = np.sum(diff ** 2)\n",
    "    return squared_error\n",
    "\n",
    "def train_glove(X, vector_size=50, iterations=50, learning_rate=0.01, clip_value=1.0):\n",
    "    vocab_size = X.shape[0]\n",
    "    W = np.random.randn(vocab_size, vector_size) / np.sqrt(vector_size)\n",
    "    U = np.random.randn(vocab_size, vector_size) / np.sqrt(vector_size)\n",
    "    b = np.zeros(vocab_size)\n",
    "    c = np.zeros(vocab_size)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        error = glove_loss(X, W, U, b, c)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}: Loss = {error}\")\n",
    "        \n",
    "        # Compute gradients\n",
    "        diff = W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1)\n",
    "        grad_W = 2 * diff.dot(U)\n",
    "        grad_U = 2 * diff.T.dot(W)\n",
    "        grad_b = 2 * np.sum(diff, axis=1)\n",
    "        grad_c = 2 * np.sum(diff, axis=0)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_W = np.clip(grad_W, -clip_value, clip_value)\n",
    "        grad_U = np.clip(grad_U, -clip_value, clip_value)\n",
    "        grad_b = np.clip(grad_b, -clip_value, clip_value)\n",
    "        grad_c = np.clip(grad_c, -clip_value, clip_value)\n",
    "        \n",
    "        # Update parameters\n",
    "        W -= learning_rate * grad_W\n",
    "        U -= learning_rate * grad_U\n",
    "        b -= learning_rate * grad_b\n",
    "        c -= learning_rate * grad_c\n",
    "    \n",
    "    return (W + U) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentence in whatsapp_df_copy['filtered_tokens']:\n",
    "    for word in sentence.split(' '):\n",
    "        corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 9547.968105155907\n",
      "Iteration 10: Loss = 886.8382972979341\n",
      "Iteration 20: Loss = 770.1583807250861\n",
      "Iteration 30: Loss = 753.4867345861778\n",
      "Iteration 40: Loss = 736.8506720763112\n",
      "Iteration 50: Loss = 722.9037751401183\n",
      "Iteration 60: Loss = 712.6764474995944\n",
      "Iteration 70: Loss = 705.9745987429415\n",
      "Iteration 80: Loss = 701.477432781958\n",
      "Iteration 90: Loss = 698.0580476327473\n"
     ]
    }
   ],
   "source": [
    "X, word_to_id = build_cooccurrence_matrix(corpus)\n",
    "word_vectors = train_glove(X, vector_size=5, iterations=100, learning_rate=0.01, clip_value=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": [-0.55860724 -0.30560315 -0.08152729 -0.27706367 -0.05147736]\n",
      "challenge: [ 0.03521697  0.02115465 -0.03628242 -0.12011233 -0.0378793 ]\n",
      "rewarding: [0.07359107 0.10351958 0.03730213 0.07946659 0.01301849]\n",
      "stranger: [ 0.06430299  0.09680638  0.07266604 -0.0337872   0.15046244]\n",
      "difference: [ 0.04797072 -0.1418214   0.16422065  0.08450146  0.04560523]\n",
      "end: [ 0.12435624  0.00549067 -0.04257162 -0.1324257   0.01576025]\n",
      "debug: [-0.00820362  0.09662148  0.06509091  0.04796182  0.03761253]\n",
      "exist: [-0.02772332  0.07703624 -0.00084299 -0.07894938 -0.0466522 ]\n",
      "cake: [-0.00324985  0.00208952  0.1046634  -0.07894299 -0.07672568]\n",
      "school: [ 0.10771834  0.00160645 -0.07924419 -0.02076776 -0.18884321]\n",
      "travel: [ 0.01400674 -0.10255247 -0.1624058   0.12760566  0.14677435]\n",
      "forward: [-0.0606057  -0.30206211 -0.053011    0.30006419  0.31107129]\n",
      "requirement: [ 0.00786203 -0.04486748 -0.03929018  0.05008567  0.07031282]\n",
      "beautiful: [-0.05965714 -0.03511297 -0.02798982 -0.11825527 -0.03675119]\n",
      "balance: [-0.01492689  0.05592068  0.14397897 -0.0365356  -0.01873208]\n",
      "super: [ 0.11327693 -0.0433765  -0.08690868  0.00813683 -0.10740595]\n",
      "little: [-0.0335358  -0.18404822  0.38535446  0.1428129  -0.02142537]\n",
      "exercise: [ 0.04523308  0.07078851 -0.00567379 -0.02776003 -0.09778018]\n",
      "haha: [ 0.01695387 -0.05953902 -0.00714816 -0.190003    0.03052036]\n",
      "definitely: [-0.43347338  0.53115381 -0.03160698  0.19388425  0.01314008]\n",
      "mean: [ 0.09619214 -0.05492206 -0.1371125   0.03319684 -0.07135055]\n",
      "mint: [ 0.02676851 -0.07390569  0.11208766 -0.07282278  0.03568508]\n",
      "miss: [-0.1844532   0.08131977 -0.04490703 -0.10620039 -0.08260858]\n",
      "watch: [-0.14489675 -0.04390991  0.08314279 -0.25748483  0.20420548]\n",
      "night: [-0.20410035  0.5312181   0.05364834  0.43151938  0.02830516]\n",
      "job: [ 0.06092636 -0.00346226  0.01177311  0.1407009  -0.19928303]\n",
      "software: [ 0.03154641 -0.09211712 -0.14627064  0.20376204 -0.02975302]\n",
      "rosemary: [-0.03049674 -0.16525459  0.04513559 -0.04638761  0.22929854]\n",
      "kind: [ 0.09740167  0.01429945  0.00541623  0.08498355 -0.09496809]\n",
      "squeeze: [ 0.0817461   0.03433963 -0.03304383 -0.07611177 -0.04301362]\n",
      "check: [-0.01663451  0.23839527  0.16514192  0.09573266  0.19044997]\n",
      "catch: [-0.52417849  0.55031684 -0.09779374  0.10968421  0.01616008]\n",
      "right: [ 0.1040211   0.0289274   0.04325623 -0.21356865  0.08181165]\n",
      "recommendation: [ 0.0786055  -0.07134331 -0.01510725 -0.12659484 -0.01417852]\n",
      "sleep: [-0.02148744  0.03303017  0.06234678  0.03218509  0.12399254]\n",
      "sure: [-0.15210241  0.27169206 -0.12438941  0.18936927  0.14645228]\n",
      "article: [ 0.04997416  0.06499538  0.00701872 -0.07100657  0.00232345]\n",
      "list: [ 0.00413057  0.1557165   0.03885281 -0.08298806  0.20932311]\n",
      "relax: [-0.30366609 -0.10822824 -0.22878144  0.04800826  0.08854357]\n",
      "lately: [-0.15636623 -0.17905427 -0.1700165   0.17503421 -0.1248185 ]\n",
      "tell: [ 0.02958118  0.00507084 -0.08362382 -0.07252609  0.00643993]\n",
      "generate: [-0.00296953 -0.08815906 -0.03074333 -0.11764399 -0.04465888]\n",
      "nightmare: [ 0.02402178 -0.02590116  0.03494387 -0.10575501 -0.0029236 ]\n",
      "tip: [ 0.05505294 -0.0583739   0.13157667 -0.0031314   0.00991551]\n",
      "love: [ 0.009925    0.14478384 -0.03988911  0.05986337  0.02090515]\n",
      "course: [-0.36890192  0.20232583 -0.04049715 -0.07405406  0.06152004]\n",
      "hand: [ 0.149444   -0.02868384  0.00983762  0.03147332  0.01459914]\n",
      "eat: [-0.17871544  0.06050115 -0.0665235  -0.11848929 -0.04432736]\n",
      "rec: [ 0.0165718   0.02883709  0.00840461 -0.09632987  0.00307646]\n",
      "drag: [-0.0839141   0.03525965  0.06846114  0.13405225  0.08747155]\n",
      "role: [-0.02808885 -0.02901596 -0.03227872 -0.1102788  -0.01794021]\n",
      "got: [ 0.01697086 -0.15244739  0.07448291  0.21982627  0.02871917]\n",
      "◊ë◊ê◊†◊í◊ú◊ô◊™: [-0.12945138 -0.18897157 -0.14404485  0.01564632  0.12469656]\n",
      "old: [ 0.04553408  0.11401908  0.06209545  0.10494603 -0.14502943]\n",
      "gripping: [ 0.04903812 -0.01844707 -0.08322304  0.01757733  0.22857263]\n",
      "ai: [ 0.05429509  0.08251752  0.01810426 -0.08588833  0.01214875]\n",
      "perfect: [0.00477381 0.10826775 0.05114883 0.19620195 0.17152426]\n",
      "upgrade: [ 0.0213277  -0.05592148 -0.07790012  0.1698476  -0.00947961]\n",
      "exit: [-0.03284978  0.02605326  0.10982096  0.03452765  0.02131037]\n",
      "noah: [ 0.10371459 -0.00446154  0.00486181 -0.12724221 -0.03753587]\n",
      "herb: [-0.01639951 -0.5376672   0.50595672  0.33419744  0.32534875]\n",
      "soon: [-0.22233339  0.42517473 -0.2035239   0.05668834 -0.15891189]\n",
      "edit: [-0.13762771 -0.05933699  0.09750078 -0.19595971  0.11323597]\n",
      "yeah: [ 0.01749831 -0.03385582  0.00092726 -0.00069162 -0.13739586]\n",
      "balcony: [ 0.04021092 -0.14017142  0.22269881  0.11790945 -0.10932584]\n",
      "tough: [ 0.05832523  0.02185175 -0.04896817  0.13474909  0.1435395 ]\n",
      "message: [-1.88951939e-01 -5.78268543e-05  6.30024624e-02 -2.18353755e-01\n",
      "  1.43275744e-01]\n",
      "bump: [-0.11230696  0.04740505 -0.01391726  0.08241691  0.13055873]\n",
      "change: [-0.01001616 -0.01631728 -0.01231311 -0.1002371  -0.00164554]\n",
      "deck: [ 0.10147962  0.02314579 -0.00213632  0.106683    0.00024777]\n",
      "like: [0.01305573 0.08208626 0.08184216 0.26749098 0.18481672]\n",
      "forget: [ 0.05690413 -0.01469348  0.04172485 -0.17425731  0.04480995]\n",
      "recipe: [ 0.11994069 -0.11536287  0.03285193 -0.00416889 -0.10211041]\n",
      "fresh: [-0.04557213 -0.29408866  0.17943228  0.40022516  0.28752151]\n",
      "chat: [ 0.09120062 -0.00550401  0.02223151 -0.12438668  0.0016451 ]\n",
      "work: [-0.05804292 -0.13218616 -0.19658786  0.39428613 -0.5305881 ]\n",
      "today: [-0.37166031 -0.08836691 -0.08338517  0.01514769  0.0932789 ]\n",
      "listen: [ 0.0224269  -0.01855342  0.15299133 -0.12374505 -0.00660242]\n",
      "hit: [ 0.08973264 -0.13146678  0.1017942  -0.00757052  0.00281064]\n",
      "noam: [-0.36136563 -0.15310308 -0.13948686 -0.10069457 -0.03164089]\n",
      "sound: [0.02713494 0.20995183 0.2125186  0.62620849 0.16003137]\n",
      "speak: [-0.00206801 -0.00391821  0.07369663 -0.05507644  0.17136589]\n",
      "provoking: [ 0.05891669  0.01467365 -0.05744971  0.00074409  0.10565264]\n",
      "gang: [ 0.05295095  0.10857336  0.0275058   0.06277839 -0.177833  ]\n",
      "add: [ 0.02767031  0.13861678  0.08916823 -0.03512562  0.08581005]\n",
      "harari: [ 0.10565625 -0.01486434 -0.00828764 -0.14789718 -0.00935328]\n",
      "ask: [ 0.12753883 -0.10108937 -0.01179335 -0.06747993 -0.15576032]\n",
      "talk: [-0.29030931  0.16688891 -0.09966716 -0.16424139 -0.01634921]\n",
      "stop: [0.01593537 0.00913097 0.01288296 0.05563655 0.17867291]\n",
      "sapiens: [ 0.09638456 -0.00507382  0.02705618 -0.12395268 -0.05145145]\n",
      "encrypt: [ 0.11613367 -0.00115884 -0.03365087 -0.13009967  0.01271504]\n",
      "recommend: [ 0.06571952 -0.00691321 -0.01641779 -0.10672532 -0.00084162]\n",
      "hey: [-0.6233295  -0.31359393 -0.14235916 -0.19540225 -0.06104493]\n",
      "enhance: [-0.07511467  0.05257966 -0.0684728  -0.13808544 -0.05333952]\n",
      "basil: [-0.0030103  -0.15760042  0.02561823  0.01597775  0.17189952]\n",
      "finish: [-0.12795046 -0.09802463  0.06827991  0.1026899   0.08185909]\n",
      "skill: [ 0.13497588 -0.24152185 -0.04238342  0.06700098 -0.07779092]\n",
      "gpt: [ 0.04319723  0.04810809 -0.0237222  -0.09466972 -0.03513574]\n",
      "struggle: [ 0.04186766 -0.02665009 -0.0009648   0.00928443 -0.01153982]\n",
      "interesting: [ 0.02471938 -0.0111822   0.12183176  0.13437929 -0.30805676]\n",
      "intense: [ 0.05575916 -0.02423098 -0.02107324  0.12439909  0.12013771]\n",
      "humankind: [ 0.02027109 -0.04605391 -0.08723649  0.08484493  0.10959147]\n",
      "learn: [-0.03136519  0.0583699   0.11767574 -0.02543274 -0.03097879]\n",
      "highly: [ 0.04442839 -0.02825631 -0.01577595 -0.12797527 -0.01137049]\n",
      "paragraph: [-0.00506128 -0.06612466 -0.03660078 -0.05588848  0.08811003]\n",
      "late: [ 0.00631942  0.17345456  0.04459155  0.07854108 -0.00968934]\n",
      "fantastic: [ 0.04686629 -0.16342824  0.20455669  0.0682522  -0.06149595]\n",
      "star: [ 0.01056767 -0.08690553 -0.17396651  0.10791188  0.1980988 ]\n",
      "anytime: [ 0.005934   -0.09667047  0.136515    0.05913761 -0.14224345]\n",
      "testing: [ 0.07547616  0.09931618  0.02812026 -0.04255634  0.06727958]\n",
      "lot: [-0.07360239 -0.03859509 -0.05813049  0.02978861  0.03230625]\n",
      "thing: [0.01406307 0.21495175 0.12365686 0.02505355 0.29124295]\n",
      "plus: [ 0.02437002 -0.19705016 -0.02453902  0.06042591  0.17738511]\n",
      "nostalgia: [ 0.01130274  0.13323749  0.18551511 -0.07912016  0.11380743]\n",
      "board: [ 0.09419715  0.08937504 -0.06901664 -0.0134155  -0.14691798]\n",
      "meet: [ 0.05017355  0.08225376 -0.0195787  -0.17397119  0.00558489]\n",
      "development: [ 0.07417182 -0.05013848 -0.05583004 -0.00715185 -0.10689254]\n",
      "emma: [ 0.02826475 -0.09345523 -0.14860534  0.10282181  0.1024299 ]\n",
      "mandalorian: [-0.19104652 -0.07516746  0.11259197 -0.16968487 -0.01116488]\n",
      "finally: [-0.05261655  0.03124493  0.14203375 -0.03436275  0.11426859]\n",
      "hobby: [ 0.0782975  -0.06654351 -0.09631955  0.11125144 -0.14733387]\n",
      "fun: [ 0.01476068  0.03550435 -0.01225098 -0.10089192 -0.03240551]\n",
      "hope: [-0.29765179 -0.1407789  -0.12448783 -0.05023475  0.00483677]\n",
      "new: [ 0.41770964 -0.14020102 -0.33047429 -0.07333349 -0.60248485]\n",
      "look: [ 0.05044434 -0.26328962  0.00468476  0.06080892  0.13381279]\n",
      "create: [ 0.07201945 -0.03517695  0.04071819 -0.16141361 -0.01969409]\n",
      "outside: [ 0.02554238 -0.13565236 -0.23341915  0.15682044  0.24957681]\n",
      "backend: [ 0.03883507  0.0213396   0.02213342  0.02101745 -0.06859037]\n",
      "place: [-0.01370035  0.08957901 -0.12960819  0.22755445  0.19135771]\n",
      "idea: [-0.38328159 -0.16524275 -0.11947794 -0.20583151 -0.04488091]\n",
      "project: [ 0.04824728  0.02013101 -0.08409814  0.52284176 -0.16185678]\n",
      "want: [ 0.00927251 -0.17838771  0.3503388   0.19932641 -0.11206587]\n",
      "yuval: [ 0.06950436 -0.00647086  0.08085674 -0.12501863 -0.02907763]\n",
      "bunch: [ 0.107595   -0.03576893 -0.06685168  0.09066707 -0.2568069 ]\n",
      "busy: [-0.1949004  -0.1185921   0.03056744 -0.05249753 -0.22643422]\n",
      "awesome: [0.03311947 0.12660133 0.18161147 0.10209595 0.11646098]\n",
      "curious: [ 0.04635428  0.11950769  0.04672384 -0.01387302 -0.02383515]\n",
      "surprisingly: [ 0.00475628 -0.1785425  -0.11037829  0.09063322  0.21398202]\n",
      "fascinating: [ 0.07300236  0.01847207  0.11812856 -0.00185115 -0.08531337]\n",
      "day: [-0.01048171 -0.01476549 -0.00639404 -0.13747299 -0.03318605]\n",
      "weekend: [0.06111255 0.14360959 0.03873244 0.03606321 0.03656057]\n",
      "therapeutic: [-0.03090983 -0.19861203 -0.00883534  0.07872492  0.22906278]\n",
      "cooking: [ 0.07520255 -0.30281553  0.27927865  0.5014639   0.04218837]\n",
      "online: [-0.01532649  0.07845959  0.07890366 -0.10010956 -0.01083333]\n",
      "believe: [ 0.09813819  0.00906577  0.00934697 -0.00097924 -0.16694491]\n",
      "processing: [ 0.10218295 -0.05102946  0.02281853  0.10024073 -0.18129364]\n",
      "write: [ 0.06685204  0.06334259 -0.00192228 -0.06484588  0.09599325]\n",
      "swamp: [ 0.1165537  -0.0630092  -0.07402027  0.0648789  -0.26220419]\n",
      "things: [ 0.06121574  0.00535329 -0.01731836 -0.02583558  0.16582443]\n",
      "garden: [-0.00972102 -0.20812002  0.2756108   0.22143622 -0.06783272]\n",
      "start: [0.01055968 0.02202403 0.21446949 0.12308767 0.06181683]\n",
      "way: [ 0.09705293 -0.05456368  0.14104992  0.12830183 -0.03812454]\n",
      "thank: [-0.31349768  0.03932955 -0.01727622 -0.08416702  0.07009664]\n",
      "bad: [-0.07960072 -0.00225171  0.22782166 -0.09988802  0.00918044]\n",
      "dive: [ 0.06610078 -0.02718255 -0.0394995  -0.01766112  0.01815721]\n",
      "deep: [ 0.09812931 -0.00581913 -0.01796093 -0.10095731 -0.02279878]\n",
      "big: [-0.07521847 -0.0724489   0.03092857  0.04417682  0.135278  ]\n",
      "group: [-0.02243083 -0.03441259  0.16303986 -0.12503895 -0.01392161]\n",
      "sign: [-0.04598749  0.1081409   0.16110594 -0.06167088  0.02394683]\n",
      "conversation: [-0.06301945 -0.15386248 -0.09722514 -0.07832608  0.04000576]\n",
      "analysis: [ 0.05814735  0.02317478  0.03216658 -0.09395869 -0.01514734]\n",
      "sam: [ 0.11442564  0.01121325 -0.0522154   0.00377188 -0.2257918 ]\n",
      "great: [-0.24178357  0.48053532  0.28228816  0.35443205  0.4718046 ]\n",
      "storyline: [-0.15191951 -0.07153809  0.03595116 -0.16877794 -0.03948261]\n",
      "◊û◊ì◊ë◊®◊ô◊ù: [-0.0649972  -0.064628    0.07889837 -0.11719077 -0.05612358]\n",
      "whatsapp: [ 0.10859671 -0.01939046 -0.00421179 -0.15870506  0.00902904]\n",
      "natural: [-0.01148418  0.03300672  0.24440049  0.02621953 -0.12157694]\n",
      "plan: [-0.20867683  0.36388138 -0.03981392  0.14501719 -0.08277212]\n",
      "good: [-1.21937569 -0.30632356 -0.38665569 -0.15781764 -0.15226576]\n",
      "grow: [ 0.05440683 -0.12793924  0.16535128  0.04944755 -0.05616694]\n",
      "amazing: [ 0.00215005  0.13661365  0.21785545 -0.01921669  0.22793069]\n",
      "care: [-0.18983721  0.26859075  0.05456329 -0.06324249  0.04363754]\n",
      "hear: [-0.08611195  0.34856027  0.1709766   0.18355373  0.21109491]\n",
      "spoon: [ 0.06139668 -0.02895536 -0.01362609 -0.15613516 -0.04259935]\n",
      "photography: [ 0.07534525  0.02014613  0.00735205 -0.01882542 -0.0193345 ]\n",
      "read: [ 0.1108066  -0.07869425  0.01091794 -0.16201647  0.04200453]\n",
      "system: [ 0.00953882 -0.05961318 -0.10704403  0.17826787  0.01820916]\n",
      "nice: [-0.00150392 -0.32085191  0.09440188  0.29301565  0.1591696 ]\n",
      "book: [ 0.04003475 -0.11861886 -0.11158686  0.17245124 -0.08315493]\n",
      "use: [-0.16764532  0.15144961 -0.04736469 -0.09351565  0.00635281]\n",
      "game: [-0.16653072  0.5453136  -0.05805911  0.34068657 -0.13635384]\n",
      "language: [ 0.08407584 -0.01656862  0.09755626 -0.06744506 -0.10674952]\n",
      "know: [0.00698636 0.22481223 0.11080847 0.10790124 0.13276654]\n",
      "office: [ 0.13497787  0.02515996 -0.03044464  0.08661068 -0.19137627]\n",
      "couple: [ 0.01585754  0.1362672   0.01877827  0.02086161 -0.19850573]\n",
      "cool: [ 0.05473194 -0.08691257 -0.15138281 -0.01651997  0.10121649]\n",
      "excited: [ 0.08395497 -0.02720585 -0.00103432 -0.08402026 -0.02560632]\n",
      "bit: [-0.01153344 -0.11942244  0.15734177  0.06180042  0.26349289]\n",
      "currently: [-0.01424431  0.10037766  0.02515233 -0.10519882 -0.05349591]\n",
      "oh: [-0.03818626  0.16023614  0.04373739  0.20407524 -0.0009453 ]\n",
      "let: [-0.27341153  0.24794163 -0.19126994  0.06954599 -0.16435373]\n",
      "suspense: [ 0.10538521  0.02595504  0.0478952  -0.22133604  0.11967286]\n",
      "problem: [-0.08507434 -0.03581252 -0.0749526  -0.00383487  0.12309089]\n",
      "setup: [-0.02027657  0.01018547  0.23299552  0.27607474 -0.00243699]\n",
      "actually: [-0.13907524 -0.10160712  0.01764312 -0.16780007 -0.0334234 ]\n",
      "try: [ 0.2384054  -0.15155682 -0.20223598  0.02077472 -0.12074216]\n",
      "gardening: [ 0.02295526 -0.06006351  0.1813368   0.03843177 -0.04832462]\n",
      "preppe: [ 0.06933597  0.02662306  0.08807742 -0.06347453 -0.00515083]\n",
      "pick: [ 0.13695582 -0.05314737 -0.08118347 -0.0536734  -0.13530975]\n",
      "maybe: [-0.06815798  0.15523108  0.05273247  0.02821673  0.12275477]\n",
      "touch: [ 0.00064668  0.15619749 -0.10831511  0.22140145 -0.39219457]\n",
      "think: [ 0.00618673  0.07227442 -0.06987365  0.05550693  0.0746946 ]\n",
      "stumble: [-0.01459011 -0.00217549 -0.06072125  0.16248559  0.08684921]\n",
      "character: [-0.09728331 -0.06782557 -0.03132119 -0.17711558 -0.03330973]\n",
      "far: [ 0.05458354 -0.13650547  0.1140056   0.0036227   0.02384963]\n",
      "hangover: [-0.10884335 -0.07757489 -0.14726896  0.02625385  0.08429795]\n",
      "wars: [ 0.06811564  0.05838311  0.00771381 -0.08882328  0.04549754]\n",
      "kid: [-0.03791381 -0.10252103 -0.14103824  0.06961902  0.12163014]\n",
      "text: [-0.23923485 -0.15526513 -0.05241871 -0.147483   -0.02720935]\n",
      "easy: [-0.10805054 -0.15530947 -0.07174092  0.28851464  0.05681113]\n",
      "afeka: [-0.04984975  0.0103281   0.1711656  -0.10921036 -0.00461647]\n",
      "come: [ 0.09669958  0.06234079  0.03826294 -0.00997774  0.03222148]\n",
      "coffee: [ 2.06648302e-02  1.10780480e-01 -5.12887544e-02 -1.67742149e-01\n",
      "  4.83411385e-05]\n",
      "remember: [ 0.04654204  0.16062576  0.02481658 -0.1002504   0.01792953]\n",
      "history: [ 0.01978398 -0.07649173 -0.16275661  0.1574245   0.12738972]\n"
     ]
    }
   ],
   "source": [
    "for word, idx in word_to_id.items():\n",
    "    print(f\"{word}: {word_vectors[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanation of applying GloVe:**\n",
    "The GloVe vectors represent semantic embeddings of the provided words. Each vector captures the meaning and context of the word in a high-dimensional space. \n",
    "\n",
    "Analysis based on the vectors:\n",
    "\n",
    "Challenge: This word has a slightly positive direction in the second dimension, suggesting a mildly positive connotation or context.  \n",
    "Rewarding: The vector indicates a positive sentiment, possibly related to satisfaction or positive outcomes.  \n",
    "Stranger: This vector shows a mix of directions, indicating complexity or varied contexts associated with the word.  \n",
    "Difference: The vector suggests a directional contrast, possibly indicating variation or distinction.  \n",
    "End: The vector shows positive directions in various dimensions, indicating completeness or finality.  \n",
    "Debug: This vector suggests a technical context, with directions pointing towards problem-solving or software debugging.  \n",
    "Exist: The vector shows a mixture, possibly indicating existence or being present.  \n",
    "Travel: This vector suggests movement or direction related to travel.  \n",
    "School: The vector points towards education or academic contexts.  \n",
    "Forward: This vector indicates directionality or progression.  \n",
    "\n",
    "Each word's vector encapsulates its semantic nuances and associations, useful for tasks like word similarity, sentiment analysis, or understanding context in natural language processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sharon's code\n",
    "# def build_cooccurrence_matrix(corpus, window_size=2):\n",
    "#     vocab = list(set(corpus))\n",
    "#     word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "#     cooccurrence = sparse.lil_matrix((len(vocab), len(vocab)), dtype=np.float64)\n",
    "    \n",
    "#     for i, word in enumerate(corpus):\n",
    "#         left_context = max(0, i - window_size)\n",
    "#         right_context = min(len(corpus), i + window_size + 1)\n",
    "#         for j in range(left_context, right_context):\n",
    "#             if i != j:\n",
    "#                 cooccurrence[word_to_id[word], word_to_id[corpus[j]]] += 1\n",
    "    \n",
    "#     return cooccurrence.tocsr(), word_to_id\n",
    "\n",
    "# def glove_loss(X, W, b, U, c):\n",
    "#     return np.sum(np.power(W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1), 2))\n",
    "\n",
    "# def train_glove(X, vector_size=50, iterations=50, learning_rate=0.05):\n",
    "#     vocab_size = X.shape[0]\n",
    "#     W = np.random.randn(vocab_size, vector_size) / np.sqrt(vector_size)\n",
    "#     U = np.random.randn(vocab_size, vector_size) / np.sqrt(vector_size)\n",
    "#     b = np.zeros(vocab_size)\n",
    "#     c = np.zeros(vocab_size)\n",
    "    \n",
    "#     for _ in range(iterations):\n",
    "#         grad_W = 2 * (W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1)).dot(U)\n",
    "#         grad_U = 2 * (W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1)).T.dot(W)\n",
    "#         grad_b = 2 * np.sum(W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1), axis=1)\n",
    "#         grad_c = 2 * np.sum(W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1), axis=0)\n",
    "        \n",
    "#         W -= learning_rate * grad_W\n",
    "#         U -= learning_rate * grad_U\n",
    "#         b -= learning_rate * grad_b\n",
    "#         c -= learning_rate * grad_c\n",
    "        \n",
    "#         if _ % 10 == 0:\n",
    "#             print(f\"Iteration {_}, Loss: {glove_loss(X, W, b, U, c)}\")\n",
    "    \n",
    "#     return (W + U) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "\n",
    "# for sentence in whatsapp_df_copy['filtered_tokens']:\n",
    "#     for word in sentence.split(' '):\n",
    "#         corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, word_to_id = build_cooccurrence_matrix(corpus)\n",
    "# word_vectors = train_glove(X, vector_size=5, iterations=100)\n",
    "\n",
    "# for word, idx in word_to_id.items():\n",
    "#     print(f\"{word}: {word_vectors[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying tagging by CYK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to show apply the CYK tagging to 5 sentences, 1 manually and 4 with libraries and built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyk_parse(sentence, grammar):\n",
    "    # Step 1: Tokenization\n",
    "    tokens = sentence.split()\n",
    "    n = len(tokens)\n",
    "    table = [[set() for _ in range(n+1)] for _ in range(n+1)]\n",
    "\t\n",
    "    # Step 2: Initialization\n",
    "    for i in range(1, n+1):\n",
    "        for rule in grammar:\n",
    "            if rule[1] == tokens[i-1]:\n",
    "                table[i][i].add(rule[0])\n",
    "\n",
    "    # Step 3: Rule Application\n",
    "    for length in range(2, n+1):\n",
    "        for i in range(1, n-length+2):\n",
    "            j = i + length - 1\n",
    "            for k in range(i, j):\n",
    "                for rule in grammar:\n",
    "                    if len(rule) == 3:\n",
    "                        for left in table[i][k]:\n",
    "                            for right in table[k+1][j]:\n",
    "                                if rule[1] in left and rule[2] in right:\n",
    "                                    table[i][j].add(rule[0])\n",
    "\n",
    "    # Step 4: Backtracking\n",
    "    if 'S' in table[1][n]:\n",
    "        return True, table\n",
    "    else:\n",
    "        return False, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Noun'}, set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Noun'}, set(), set(), set(), set()], [set(), set(), {'Verb'}, set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Noun'}, set(), set(), set(), set()], [set(), set(), {'Verb'}, set(), set(), set()], [set(), set(), set(), {'Det'}, set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Noun'}, set(), set(), set(), set()], [set(), set(), {'Verb'}, set(), set(), set()], [set(), set(), set(), {'Det'}, set(), set()], [set(), set(), set(), set(), {'adjective'}, set()], [set(), set(), set(), set(), set(), set()]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Noun'}, set(), set(), set(), set()], [set(), set(), {'Verb'}, set(), set(), set()], [set(), set(), set(), {'Det'}, set(), set()], [set(), set(), set(), set(), {'adjective'}, set()], [set(), set(), set(), set(), set(), {'Noun'}]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Noun'}, set(), set(), set(), set()], [set(), set(), {'Verb'}, set(), set(), set()], [set(), set(), set(), {'Det'}, set(), set()], [set(), set(), set(), set(), {'adjective'}, {'NP'}], [set(), set(), set(), set(), set(), {'Noun'}]]\n",
      "Input sentence:   today was a beautiful day\n",
      "[set(), set(), set(), set(), set(), set()]\n",
      "[set(), {'Noun'}, set(), set(), set(), set()]\n",
      "[set(), set(), {'Verb'}, set(), set(), set()]\n",
      "[set(), set(), set(), {'Det'}, set(), set()]\n",
      "[set(), set(), set(), set(), {'adjective'}, {'NP'}]\n",
      "[set(), set(), set(), set(), set(), {'Noun'}]\n",
      "Sentence not parsed.\n"
     ]
    }
   ],
   "source": [
    "# Creating the Grammar & table manually\n",
    "sentence = whatsapp_df_copy['whatsapp_text'][19]\n",
    "\n",
    "grammar = [\n",
    "    ('S', 'NP', 'VP'),\n",
    "    ('NP', 'DET', 'NP'),\n",
    "    ('NP', 'adjective', 'Noun'),\n",
    "    ('VP', 'Verb', 'NP'),\n",
    "    ('Noun', 'today'),\n",
    "    ('Noun', 'day'),\n",
    "    ('Verb', 'was'),\n",
    "    ('Det', 'a'),\n",
    "    ('adjective', 'beautiful')\n",
    "]\n",
    "\n",
    "# Call the CYK parser\n",
    "parsed, table = cyk_parse(sentence, grammar)\n",
    "\n",
    "# Print the parse table and whether the sentence was parsed or not\n",
    "if parsed:\n",
    "    print(\"Input sentence: \", sentence)\n",
    "    print(\"Parse table: \")\n",
    "    for row in table:\n",
    "        print(row)\n",
    "else:\n",
    "    print(\"Input sentence: \", sentence)\n",
    "    for row in table:\n",
    "        print(row)\n",
    "    print(\"Sentence not parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the sentence\n",
    "tokens = [\"today\", \"was\", \"a\", \"beautiful\", \"day\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Noun'], [], [], [], []]\n",
      "[[], ['Verb'], [], [], []]\n",
      "[[], [], ['Det'], [], []]\n",
      "[[], [], [], ['adjective'], ['NP']]\n",
      "[[], [], [], [], ['Noun']]\n"
     ]
    }
   ],
   "source": [
    "# initializing the cyk table\n",
    "n = len(sentence)\n",
    "cyk_table = [[[] for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "for j in range(n):\n",
    "    word = sentence[j]\n",
    "    for rule in grammar:\n",
    "        if len(rule) == 2 and rule[1] == word:\n",
    "            cyk_table[j][j].append(rule[0])\n",
    "\n",
    "# Fill the CYK table for lengths > 1\n",
    "for length in range(2, n + 1):\n",
    "    for i in range(n - length + 1):\n",
    "        j = i + length - 1\n",
    "        for k in range(i, j):\n",
    "            for rule in grammar:\n",
    "                if len(rule) == 3:\n",
    "                    lhs, rhs1, rhs2 = rule\n",
    "                    if rhs1 in cyk_table[i][k] and rhs2 in cyk_table[k + 1][j]:\n",
    "                        cyk_table[i][j].append(lhs)\n",
    "\n",
    "# Print the CYK table\n",
    "for row in cyk_table:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYK Table:\n",
      "['{Noun}', '{}', '{}', '{}', '{}']\n",
      "['{}', '{Verb}', '{}', '{}', '{VP}']\n",
      "['{}', '{}', '{Det}', '{}', '{NP}']\n",
      "['{}', '{}', '{}', '{adjective}', '{NP}']\n",
      "['{}', '{}', '{}', '{}', '{Noun}']\n",
      "\n",
      "The sentence cannot be derived from the start symbol 'S'.\n"
     ]
    }
   ],
   "source": [
    "def cyk_parse(grammar, sentence):\n",
    "    # Length of the sentence\n",
    "    n = len(sentence)\n",
    "\n",
    "    # Initialize the CYK table\n",
    "    cyk_table = [[set() for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    # Fill the table for length 1 substrings (single words)\n",
    "    for j in range(n):\n",
    "        word = sentence[j]\n",
    "        for rule in grammar:\n",
    "            if len(rule) == 2 and rule[1] == word:\n",
    "                cyk_table[j][j].add(rule[0])\n",
    "\n",
    "    # Fill the table for substrings of length 2 to n\n",
    "    for length in range(2, n + 1):\n",
    "        for i in range(n - length + 1):\n",
    "            j = i + length - 1\n",
    "            for k in range(i, j):\n",
    "                for rule in grammar:\n",
    "                    if len(rule) == 3:\n",
    "                        A, B, C = rule\n",
    "                        if B in cyk_table[i][k] and C in cyk_table[k + 1][j]:\n",
    "                            cyk_table[i][j].add(A)\n",
    "\n",
    "    # Print the CYK table\n",
    "    print(\"CYK Table:\")\n",
    "    for row in cyk_table:\n",
    "        print([\"{\" + \", \".join(cell) + \"}\" for cell in row])\n",
    "\n",
    "    # Check if the start symbol 'S' is in the top-right cell of the table\n",
    "    if 'S' in cyk_table[0][n - 1]:\n",
    "        print(\"\\nThe sentence can be derived from the start symbol 'S'.\")\n",
    "    else:\n",
    "        print(\"\\nThe sentence cannot be derived from the start symbol 'S'.\")\n",
    "\n",
    "# Define the grammar rules\n",
    "grammar = [\n",
    "    ('S', 'NP', 'VP'),\n",
    "    ('NP', 'Noun'),\n",
    "    ('NP', 'Det', 'NP'),\n",
    "    ('NP', 'adjective', 'Noun'),\n",
    "    ('VP', 'Verb', 'NP'),\n",
    "    ('Noun', 'today'),\n",
    "    ('Noun', 'day'),\n",
    "    ('Verb', 'was'),\n",
    "    ('Det', 'a'),\n",
    "    ('adjective', 'beautiful')\n",
    "]\n",
    "\n",
    "# Define the sentence\n",
    "sentence = [\"today\", \"was\", \"a\", \"beautiful\", \"day\"]\n",
    "\n",
    "# Run the CYK parser\n",
    "cyk_parse(grammar, sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parsed, table = cyk_parse(sentence, grammar)\n",
    "\n",
    "# Print the parse table and whether the sentence was parsed or not\n",
    "if parsed:\n",
    "    print(\"Input sentence: \", sentence)\n",
    "    print(\"Parse table: \")\n",
    "    for row in table:\n",
    "        print(row)\n",
    "else:\n",
    "    print(\"Input sentence: \", sentence)\n",
    "    print(\"Sentence not parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, set(), set(), set(), set()], [set(), set(), {'Noun'}, set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, set(), set(), set(), set()], [set(), set(), {'Noun'}, set(), set(), set()], [set(), set(), set(), {'Verb'}, set(), set()], [set(), set(), set(), set(), set(), set()], [set(), set(), set(), set(), set(), set()]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, set(), set(), set(), set()], [set(), set(), {'Noun'}, set(), set(), set()], [set(), set(), set(), {'Verb'}, set(), set()], [set(), set(), set(), set(), {'Det'}, set()], [set(), set(), set(), set(), set(), set()]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, set(), set(), set(), set()], [set(), set(), {'Noun'}, set(), set(), set()], [set(), set(), set(), {'Verb'}, set(), set()], [set(), set(), set(), set(), {'Det'}, set()], [set(), set(), set(), set(), set(), {'Noun'}]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, {'NP'}, set(), set(), set()], [set(), set(), {'Noun'}, set(), set(), set()], [set(), set(), set(), {'Verb'}, set(), set()], [set(), set(), set(), set(), {'Det'}, set()], [set(), set(), set(), set(), set(), {'Noun'}]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, {'NP'}, set(), set(), set()], [set(), set(), {'Noun'}, set(), set(), set()], [set(), set(), set(), {'Verb'}, set(), set()], [set(), set(), set(), set(), {'Det'}, {'NP'}], [set(), set(), set(), set(), set(), {'Noun'}]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, {'NP'}, set(), set(), set()], [set(), set(), {'Noun'}, set(), set(), set()], [set(), set(), set(), {'Verb'}, set(), {'VP'}], [set(), set(), set(), set(), {'Det'}, {'NP'}], [set(), set(), set(), set(), set(), {'Noun'}]]\n",
      "[[set(), set(), set(), set(), set(), set()], [set(), {'Det'}, {'NP'}, set(), set(), {'S'}], [set(), set(), {'Noun'}, set(), set(), set()], [set(), set(), set(), {'Verb'}, set(), {'VP'}], [set(), set(), set(), set(), {'Det'}, {'NP'}], [set(), set(), set(), set(), set(), {'Noun'}]]\n",
      "Input sentence:  the cat chased a dog\n",
      "Parse table: \n",
      "[set(), set(), set(), set(), set(), set()]\n",
      "[set(), {'Det'}, {'NP'}, set(), set(), {'S'}]\n",
      "[set(), set(), {'Noun'}, set(), set(), set()]\n",
      "[set(), set(), set(), {'Verb'}, set(), {'VP'}]\n",
      "[set(), set(), set(), set(), {'Det'}, {'NP'}]\n",
      "[set(), set(), set(), set(), set(), {'Noun'}]\n"
     ]
    }
   ],
   "source": [
    "# Define the context-free grammar in CNF\n",
    "grammar = [\n",
    "    ('S', 'NP', 'VP'),\n",
    "    ('NP', 'Det', 'Noun'),\n",
    "    ('VP', 'Verb', 'NP'),\n",
    "    ('Det', 'the'),\n",
    "    ('Det', 'a'),\n",
    "    ('Noun', 'cat'),\n",
    "    ('Noun', 'dog'),\n",
    "    ('Verb', 'chased'),\n",
    "    ('Verb', 'ate')\n",
    "]\n",
    "\n",
    "# Input sentence to be parsed\n",
    "sentence = \"the cat chased a dog\"\n",
    "\n",
    "# Call the CYK parser\n",
    "parsed, table = cyk_parse(sentence, grammar)\n",
    "\n",
    "# Print the parse table and whether the sentence was parsed or not\n",
    "if parsed:\n",
    "    print(\"Input sentence: \", sentence)\n",
    "    print(\"Parse table: \")\n",
    "    for row in table:\n",
    "        print(row)\n",
    "else:\n",
    "    print(\"Input sentence: \", sentence)\n",
    "    print(\"Sentence not parsed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
