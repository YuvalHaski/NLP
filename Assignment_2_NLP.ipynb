{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 330.3 kB/s eta 0:00:39\n",
      "     --------------------------------------- 0.0/12.8 MB 281.8 kB/s eta 0:00:46\n",
      "     --------------------------------------- 0.1/12.8 MB 525.1 kB/s eta 0:00:25\n",
      "      -------------------------------------- 0.2/12.8 MB 748.1 kB/s eta 0:00:17\n",
      "      -------------------------------------- 0.3/12.8 MB 927.4 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 0.7/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.9/12.8 MB 2.0 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.2/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.3 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.1/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.2/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.5/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.7/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.7/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.8/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 2.9/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.0/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.1/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.2/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.4/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.0/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.2/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.7/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.8/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 7.0/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.7/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.2/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.5/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.8/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.3/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.6/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.8/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.9/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.1/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 3.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.2/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.4)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.17.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install pandas\n",
    "!pip install -U pip setuptools wheel\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install openpyxl\n",
    "!pip install scikit-learn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing the text**\n",
    "The steps will be: \n",
    "* Tokenization\n",
    "* Lemmatization\n",
    "* Cleaning the Lemmatized tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Functions (using spaCy) for Tokenization, Lemmatization & Cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def lemmatize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "def clean_text_spacy(tokens):\n",
    "    cleaned_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will be using a Whatsapp conversation as our corpus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_chat.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "data = []\n",
    "for row in content.split('\\n'):\n",
    "    data.append(row[row.rfind(':') + 1:])\n",
    "\n",
    "whatsapp_df = pd.DataFrame({\"whatsapp_text\":data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Applying Tokenization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsapp_df['scraped_tokens'] = whatsapp_df['whatsapp_text'].apply(tokenize_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Applying Lemmatization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsapp_df['scraped_lemmatization'] = whatsapp_df['whatsapp_text'].apply(lemmatize_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cleaning the lemmatized tokens*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsapp_df['cleaned_tokens'] = whatsapp_df['scraped_lemmatization'].apply(clean_text_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsapp_df['filtered_tokens'] = whatsapp_df['cleaned_tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>whatsapp_text</th>\n",
       "      <th>scraped_tokens</th>\n",
       "      <th>scraped_lemmatization</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚ÄéMessages and calls are end-to-end encrypted....</td>\n",
       "      <td>[ , ‚ÄéMessages, and, calls, are, end, -, to, -,...</td>\n",
       "      <td>[ , ‚Äémessage, and, call, be, end, -, to, -, en...</td>\n",
       "      <td>[and, call, be, end, to, end, encrypt, no, one...</td>\n",
       "      <td>end end encrypt outside chat whatsapp read listen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚ÄéYou created group ‚Äú◊û◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊†◊í◊ú◊ô◊™‚Äù</td>\n",
       "      <td>[ , ‚ÄéYou, created, group, ‚Äú, ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™, ‚Äù]</td>\n",
       "      <td>[ , ‚Äéyou, create, group, \", ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™, \"]</td>\n",
       "      <td>[create, group, ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™]</td>\n",
       "      <td>create group ◊û◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊†◊í◊ú◊ô◊™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey noam how are you?</td>\n",
       "      <td>[ , hey, noam, how, are, you, ?]</td>\n",
       "      <td>[ , hey, noam, how, be, you, ?]</td>\n",
       "      <td>[hey, noam, how, be, you]</td>\n",
       "      <td>hey noam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey!! I‚Äôm good thanks how about you</td>\n",
       "      <td>[ , Hey, !, !, I, ‚Äôm, good, thanks, how, about...</td>\n",
       "      <td>[ , Hey, !, !, I, ‚Äôm, good, thank, how, about,...</td>\n",
       "      <td>[hey, i, good, thank, how, about, you]</td>\n",
       "      <td>hey good thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are you going to do today</td>\n",
       "      <td>[ , What, are, you, going, to, do, today]</td>\n",
       "      <td>[ , what, be, you, go, to, do, today]</td>\n",
       "      <td>[what, be, you, go, to, do, today]</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       whatsapp_text  \\\n",
       "0   ‚ÄéMessages and calls are end-to-end encrypted....   \n",
       "1                ‚ÄéYou created group ‚Äú◊û◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊†◊í◊ú◊ô◊™‚Äù   \n",
       "2                              hey noam how are you?   \n",
       "3                Hey!! I‚Äôm good thanks how about you   \n",
       "4                     What are you going to do today   \n",
       "\n",
       "                                      scraped_tokens  \\\n",
       "0  [ , ‚ÄéMessages, and, calls, are, end, -, to, -,...   \n",
       "1   [ , ‚ÄéYou, created, group, ‚Äú, ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™, ‚Äù]   \n",
       "2                   [ , hey, noam, how, are, you, ?]   \n",
       "3  [ , Hey, !, !, I, ‚Äôm, good, thanks, how, about...   \n",
       "4          [ , What, are, you, going, to, do, today]   \n",
       "\n",
       "                               scraped_lemmatization  \\\n",
       "0  [ , ‚Äémessage, and, call, be, end, -, to, -, en...   \n",
       "1    [ , ‚Äéyou, create, group, \", ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™, \"]   \n",
       "2                    [ , hey, noam, how, be, you, ?]   \n",
       "3  [ , Hey, !, !, I, ‚Äôm, good, thank, how, about,...   \n",
       "4              [ , what, be, you, go, to, do, today]   \n",
       "\n",
       "                                      cleaned_tokens  \\\n",
       "0  [and, call, be, end, to, end, encrypt, no, one...   \n",
       "1                   [create, group, ◊û◊ì◊ë◊®◊ô◊ù, ◊ë◊ê◊†◊í◊ú◊ô◊™]   \n",
       "2                          [hey, noam, how, be, you]   \n",
       "3             [hey, i, good, thank, how, about, you]   \n",
       "4                 [what, be, you, go, to, do, today]   \n",
       "\n",
       "                                     filtered_tokens  \n",
       "0  end end encrypt outside chat whatsapp read listen  \n",
       "1                        create group ◊û◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊†◊í◊ú◊ô◊™  \n",
       "2                                           hey noam  \n",
       "3                                     hey good thank  \n",
       "4                                              today  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatsapp_df_copy = whatsapp_df.copy()\n",
    "whatsapp_df_copy['filtered_tokens'] = whatsapp_df_copy['filtered_tokens'].apply(lambda x: ' '.join(x))\n",
    "whatsapp_df_copy.to_excel('whatsapp_df.xlsx')\n",
    "whatsapp_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying Feature Extraction by the following algorithms:**\n",
    "* BOW\n",
    "* TF-IDF\n",
    "* Word embedding by WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bow algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_extraction(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X.toarray(), vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Features:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Feature Names:\n",
      " ['actually' 'add' 'afeka' 'ai' 'amazing' 'analysis' 'anytime' 'article'\n",
      " 'ask' 'awesome' 'backend' 'bad' 'balance' 'balcony' 'basil' 'beach'\n",
      " 'beautiful' 'believe' 'big' 'bit' 'board' 'book' 'brightly' 'bump'\n",
      " 'bunch' 'busy' 'care' 'case' 'catch' 'challenge' 'change' 'character'\n",
      " 'chat' 'check' 'child' 'clever' 'coffee' 'come' 'conversation' 'cooking'\n",
      " 'cool' 'couple' 'course' 'cream' 'create' 'curious' 'currently' 'day'\n",
      " 'debug' 'deck' 'deep' 'definitely' 'detective' 'development' 'difference'\n",
      " 'dive' 'drag' 'early' 'easy' 'eat' 'edit' 'emma' 'encrypt' 'end'\n",
      " 'enhance' 'excited' 'exercise' 'exist' 'exit' 'fantastic' 'far'\n",
      " 'fascinating' 'finally' 'finish' 'forget' 'forward' 'fresh' 'friend'\n",
      " 'fun' 'game' 'gang' 'garden' 'gardening' 'generate' 'good' 'got' 'gpt'\n",
      " 'great' 'gripping' 'ground' 'group' 'grow' 'haha' 'hand' 'hangover'\n",
      " 'harari' 'hear' 'herb' 'hey' 'highly' 'history' 'hit' 'hobby' 'hope'\n",
      " 'humankind' 'ice' 'idea' 'intense' 'interesting' 'job' 'kid' 'kind'\n",
      " 'know' 'language' 'late' 'lately' 'learn' 'let' 'like' 'list' 'listen'\n",
      " 'little' 'look' 'lot' 'love' 'mandalorian' 'maybe' 'mean' 'meet'\n",
      " 'message' 'mint' 'miss' 'mysterious' 'natural' 'new' 'nice' 'night'\n",
      " 'nightmare' 'noah' 'noam' 'nostalgia' 'office' 'oh' 'old' 'online'\n",
      " 'outside' 'paragraph' 'perfect' 'photography' 'pick' 'place' 'plan'\n",
      " 'plus' 'preppe' 'problem' 'processing' 'project' 'provoking' 'read' 'rec'\n",
      " 'recipe' 'recommend' 'recommendation' 'relax' 'remember' 'requirement'\n",
      " 'rewarding' 'right' 'role' 'rosemary' 'sam' 'sapiens' 'school' 'shine'\n",
      " 'sign' 'skill' 'sleep' 'software' 'solve' 'soon' 'sound' 'speak'\n",
      " 'squeeze' 'star' 'start' 'stop' 'storyline' 'stranger' 'struggle'\n",
      " 'stumble' 'sun' 'super' 'sure' 'surprisingly' 'suspense' 'swamp' 'system'\n",
      " 'talk' 'tell' 'testing' 'text' 'thank' 'therapeutic' 'thing' 'things'\n",
      " 'think' 'tip' 'today' 'touch' 'tough' 'travel' 'try' 'upgrade' 'use'\n",
      " 'want' 'warm' 'wars' 'watch' 'way' 'weekend' 'whatsapp' 'work' 'write'\n",
      " 'yeah' 'yuval' '◊ë◊ê◊†◊í◊ú◊ô◊™' '◊û◊ì◊ë◊®◊ô◊ù']\n"
     ]
    }
   ],
   "source": [
    "bow_features, bow_feature_names = bow_extraction(whatsapp_df_copy['filtered_tokens'])\n",
    "print(\"BoW Features:\\n\", bow_features)\n",
    "print(\"Feature Names:\\n\", bow_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_extraction(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X.toarray(), vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Features:\n",
      " [[0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.5 0.5]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]]\n",
      "Feature Names:\n",
      " ['actually' 'add' 'afeka' 'ai' 'amazing' 'analysis' 'anytime' 'article'\n",
      " 'ask' 'awesome' 'backend' 'bad' 'balance' 'balcony' 'basil' 'beach'\n",
      " 'beautiful' 'believe' 'big' 'bit' 'board' 'book' 'brightly' 'bump'\n",
      " 'bunch' 'busy' 'care' 'case' 'catch' 'challenge' 'change' 'character'\n",
      " 'chat' 'check' 'child' 'clever' 'coffee' 'come' 'conversation' 'cooking'\n",
      " 'cool' 'couple' 'course' 'cream' 'create' 'curious' 'currently' 'day'\n",
      " 'debug' 'deck' 'deep' 'definitely' 'detective' 'development' 'difference'\n",
      " 'dive' 'drag' 'early' 'easy' 'eat' 'edit' 'emma' 'encrypt' 'end'\n",
      " 'enhance' 'excited' 'exercise' 'exist' 'exit' 'fantastic' 'far'\n",
      " 'fascinating' 'finally' 'finish' 'forget' 'forward' 'fresh' 'friend'\n",
      " 'fun' 'game' 'gang' 'garden' 'gardening' 'generate' 'good' 'got' 'gpt'\n",
      " 'great' 'gripping' 'ground' 'group' 'grow' 'haha' 'hand' 'hangover'\n",
      " 'harari' 'hear' 'herb' 'hey' 'highly' 'history' 'hit' 'hobby' 'hope'\n",
      " 'humankind' 'ice' 'idea' 'intense' 'interesting' 'job' 'kid' 'kind'\n",
      " 'know' 'language' 'late' 'lately' 'learn' 'let' 'like' 'list' 'listen'\n",
      " 'little' 'look' 'lot' 'love' 'mandalorian' 'maybe' 'mean' 'meet'\n",
      " 'message' 'mint' 'miss' 'mysterious' 'natural' 'new' 'nice' 'night'\n",
      " 'nightmare' 'noah' 'noam' 'nostalgia' 'office' 'oh' 'old' 'online'\n",
      " 'outside' 'paragraph' 'perfect' 'photography' 'pick' 'place' 'plan'\n",
      " 'plus' 'preppe' 'problem' 'processing' 'project' 'provoking' 'read' 'rec'\n",
      " 'recipe' 'recommend' 'recommendation' 'relax' 'remember' 'requirement'\n",
      " 'rewarding' 'right' 'role' 'rosemary' 'sam' 'sapiens' 'school' 'shine'\n",
      " 'sign' 'skill' 'sleep' 'software' 'solve' 'soon' 'sound' 'speak'\n",
      " 'squeeze' 'star' 'start' 'stop' 'storyline' 'stranger' 'struggle'\n",
      " 'stumble' 'sun' 'super' 'sure' 'surprisingly' 'suspense' 'swamp' 'system'\n",
      " 'talk' 'tell' 'testing' 'text' 'thank' 'therapeutic' 'thing' 'things'\n",
      " 'think' 'tip' 'today' 'touch' 'tough' 'travel' 'try' 'upgrade' 'use'\n",
      " 'want' 'warm' 'wars' 'watch' 'way' 'weekend' 'whatsapp' 'work' 'write'\n",
      " 'yeah' 'yuval' '◊ë◊ê◊†◊í◊ú◊ô◊™' '◊û◊ì◊ë◊®◊ô◊ù']\n"
     ]
    }
   ],
   "source": [
    "tfidf_features, tfidf_feature_names = tfidf_extraction(whatsapp_df_copy['filtered_tokens'])\n",
    "print(\"TF-IDF Features:\\n\", tfidf_features)\n",
    "print(\"Feature Names:\\n\", tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embeddings by Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_extraction(tokens_list, vector_size=100, window=5, min_count=1, workers=4):\n",
    "    model = Word2Vec(sentences=tokens_list, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('relax', 0.22336749732494354), ('enhance', 0.22177574038505554), ('system', 0.19541819393634796), ('forget', 0.19149678945541382), ('miss', 0.19064918160438538), ('character', 0.1868743598461151), ('kind', 0.17510007321834564), ('noah', 0.1748739331960678), ('plus', 0.1747390180826187), ('debug', 0.16881349682807922)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [text.split() for text in whatsapp_df_copy['filtered_tokens']]\n",
    "\n",
    "word2vec_model = word2vec_extraction(tokenized_texts)\n",
    "similar_words = word2vec_model.wv.most_similar('finish')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Glove Explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations (embeddings) for words. These embeddings capture semantic relationships between words based on their co-occurrence statistics in a large corpus of text. Unlike Word2Vec, which learns embeddings by predicting context words given a target word (skip-gram model) or predicting a target word given context words (continuous bag of words model), GloVe learns embeddings by factorizing the word co-occurrence matrix.\n",
    "\n",
    "Explanation of GloVe:\n",
    "* Co-occurrence Matrix: GloVe starts with a word-word co-occurrence matrix where each element \n",
    "ùëãùëñùëó ‚Äãrepresents how often word ùëó appears in the context of word ùëñ within a fixed window size.\n",
    "\n",
    "* Objective: The goal of GloVe is to learn word embeddings such that the dot product of two word vectors corresponds to the logarithm of their co-occurrence probability.\n",
    "\n",
    "* Training: GloVe uses stochastic gradient descent to minimize a loss function that measures the discrepancy between the dot product of word vectors and the logarithm of their co-occurrence probabilities.\n",
    "\n",
    "* Advantages: GloVe embeddings tend to capture global semantic meanings better than some other models, and they often perform well in tasks requiring understanding of word relationships and analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, window_size=2):\n",
    "    vocab = list(set(corpus))\n",
    "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    cooccurrence = lil_matrix((len(vocab), len(vocab)), dtype=np.float64)\n",
    "    \n",
    "    for i, word in enumerate(corpus):\n",
    "        left_context = max(0, i - window_size)\n",
    "        right_context = min(len(corpus), i + window_size + 1)\n",
    "        for j in range(left_context, right_context):\n",
    "            if i != j:\n",
    "                cooccurrence[word_to_id[word], word_to_id[corpus[j]]] += 1\n",
    "    \n",
    "    return cooccurrence.tocsr(), word_to_id\n",
    "\n",
    "def glove_loss(X, W, U, b, c):\n",
    "    diff = W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1)\n",
    "    squared_error = np.sum(diff ** 2)\n",
    "    return squared_error\n",
    "\n",
    "def train_glove(X, vector_size=50, iterations=50, learning_rate=0.01):\n",
    "    vocab_size = X.shape[0]\n",
    "    W = np.random.randn(vocab_size, vector_size) / np.sqrt(vector_size)\n",
    "    U = np.random.randn(vocab_size, vector_size) / np.sqrt(vector_size)\n",
    "    b = np.zeros(vocab_size)\n",
    "    c = np.zeros(vocab_size)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        error = glove_loss(X, W, U, b, c)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}: Loss = {error}\")\n",
    "        \n",
    "        # Compute gradients\n",
    "        diff = W.dot(U.T) + b[:, np.newaxis] + c[np.newaxis, :] - np.log(X.toarray() + 1)\n",
    "        grad_W = 2 * diff.dot(U)\n",
    "        grad_U = 2 * diff.T.dot(W)\n",
    "        grad_b = 2 * np.sum(diff, axis=1)\n",
    "        grad_c = 2 * np.sum(diff, axis=0)\n",
    "                \n",
    "        # Update parameters\n",
    "        W -= learning_rate * grad_W\n",
    "        U -= learning_rate * grad_U\n",
    "        b -= learning_rate * grad_b\n",
    "        c -= learning_rate * grad_c\n",
    "    \n",
    "    return (W + U) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentence in whatsapp_df_copy['filtered_tokens']:\n",
    "    for word in sentence.split(' '):\n",
    "        corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 10813.924315876615\n",
      "Iteration 10: Loss = 1763.977827187113\n",
      "Iteration 20: Loss = 1051.5035496490389\n",
      "Iteration 30: Loss = 850.5703121914399\n",
      "Iteration 40: Loss = 768.8830422611537\n",
      "Iteration 50: Loss = 729.1541170285084\n",
      "Iteration 60: Loss = 707.5929587796551\n",
      "Iteration 70: Loss = 694.924093814273\n",
      "Iteration 80: Loss = 686.9770883930474\n",
      "Iteration 90: Loss = 681.6932304954524\n",
      "Iteration 100: Loss = 677.9854997460309\n",
      "Iteration 110: Loss = 675.2482256747436\n",
      "Iteration 120: Loss = 673.127778079459\n",
      "Iteration 130: Loss = 671.40867098846\n",
      "Iteration 140: Loss = 669.9542759575444\n",
      "Iteration 150: Loss = 668.6747088007278\n",
      "Iteration 160: Loss = 667.5087978801464\n",
      "Iteration 170: Loss = 666.4136157264198\n",
      "Iteration 180: Loss = 665.3582131209791\n",
      "Iteration 190: Loss = 664.3197674137193\n",
      "Iteration 200: Loss = 663.2811643425039\n",
      "Iteration 210: Loss = 662.2294586636441\n",
      "Iteration 220: Loss = 661.154889391882\n",
      "Iteration 230: Loss = 660.0502530134871\n",
      "Iteration 240: Loss = 658.9105100384893\n",
      "Iteration 250: Loss = 657.7325415844589\n",
      "Iteration 260: Loss = 656.5149968509527\n",
      "Iteration 270: Loss = 655.2581870109921\n",
      "Iteration 280: Loss = 653.963990780132\n",
      "Iteration 290: Loss = 652.6357445629601\n",
      "Iteration 300: Loss = 651.2780973700354\n",
      "Iteration 310: Loss = 649.8968185689705\n",
      "Iteration 320: Loss = 648.4985551794085\n",
      "Iteration 330: Loss = 647.0905443898497\n",
      "Iteration 340: Loss = 645.6802953040927\n",
      "Iteration 350: Loss = 644.2752604026576\n",
      "Iteration 360: Loss = 642.8825207053063\n",
      "Iteration 370: Loss = 641.5085084604054\n",
      "Iteration 380: Loss = 640.1587873706708\n",
      "Iteration 390: Loss = 638.8379036424153\n",
      "Iteration 400: Loss = 637.5493128419207\n",
      "Iteration 410: Loss = 636.2953792204448\n",
      "Iteration 420: Loss = 635.0774372385146\n",
      "Iteration 430: Loss = 633.8959004259282\n",
      "Iteration 440: Loss = 632.7504007927052\n",
      "Iteration 450: Loss = 631.6399425253736\n",
      "Iteration 460: Loss = 630.5630560368161\n",
      "Iteration 470: Loss = 629.517941797558\n",
      "Iteration 480: Loss = 628.5025970154098\n",
      "Iteration 490: Loss = 627.5149215784891\n",
      "Iteration 500: Loss = 626.5528023906259\n",
      "Iteration 510: Loss = 625.6141771764276\n",
      "Iteration 520: Loss = 624.6970800425448\n",
      "Iteration 530: Loss = 623.7996716727839\n",
      "Iteration 540: Loss = 622.9202571662938\n",
      "Iteration 550: Loss = 622.0572943567247\n",
      "Iteration 560: Loss = 621.2093951080685\n",
      "Iteration 570: Loss = 620.3753216688323\n",
      "Iteration 580: Loss = 619.5539797468283\n",
      "Iteration 590: Loss = 618.7444095816974\n",
      "Iteration 600: Loss = 617.9457759607172\n",
      "Iteration 610: Loss = 617.1573578514137\n",
      "Iteration 620: Loss = 616.3785381097335\n",
      "Iteration 630: Loss = 615.6087935586152\n",
      "Iteration 640: Loss = 614.8476856105794\n",
      "Iteration 650: Loss = 614.0948515212449\n",
      "Iteration 660: Loss = 613.3499963009897\n",
      "Iteration 670: Loss = 612.6128852729033\n",
      "Iteration 680: Loss = 611.8833372414454\n",
      "Iteration 690: Loss = 611.1612182236523\n",
      "Iteration 700: Loss = 610.4464356899972\n",
      "Iteration 710: Loss = 609.7389332626127\n",
      "Iteration 720: Loss = 609.0386858225726\n",
      "Iteration 730: Loss = 608.3456949838876\n",
      "Iteration 740: Loss = 607.6599848987194\n",
      "Iteration 750: Loss = 606.9815983653018\n",
      "Iteration 760: Loss = 606.3105932166379\n",
      "Iteration 770: Loss = 605.6470389738727\n",
      "Iteration 780: Loss = 604.9910137531118\n",
      "Iteration 790: Loss = 604.342601418261\n",
      "Iteration 800: Loss = 603.7018889752062\n",
      "Iteration 810: Loss = 603.068964204379\n",
      "Iteration 820: Loss = 602.4439135295471\n",
      "Iteration 830: Loss = 601.8268201206974\n",
      "Iteration 840: Loss = 601.217762228244\n",
      "Iteration 850: Loss = 600.6168117446822\n",
      "Iteration 860: Loss = 600.0240329883652\n",
      "Iteration 870: Loss = 599.4394817024352\n",
      "Iteration 880: Loss = 598.8632042602375\n",
      "Iteration 890: Loss = 598.2952370668919\n",
      "Iteration 900: Loss = 597.7356061451712\n",
      "Iteration 910: Loss = 597.1843268925253\n",
      "Iteration 920: Loss = 596.6414039950256\n",
      "Iteration 930: Loss = 596.1068314832306\n",
      "Iteration 940: Loss = 595.5805929144882\n",
      "Iteration 950: Loss = 595.0626616660034\n",
      "Iteration 960: Loss = 594.5530013230832\n",
      "Iteration 970: Loss = 594.051566147315\n",
      "Iteration 980: Loss = 593.5583016099874\n",
      "Iteration 990: Loss = 593.073144976821\n"
     ]
    }
   ],
   "source": [
    "X, word_to_id = build_cooccurrence_matrix(corpus)\n",
    "word_vectors = train_glove(X, vector_size=5, iterations=1000, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": [ 0.32666741  0.39292591 -0.02530403  0.23449565 -0.616863  ]\n",
      "plus: [0.10353349 0.03651708 0.14362765 0.03163492 0.02704779]\n",
      "ai: [-0.07670163  0.10681701 -0.03908815 -0.09864592 -0.02434656]\n",
      "outside: [0.02235492 0.03203921 0.14403791 0.08965167 0.00115693]\n",
      "herb: [ 3.47237382e-01 -2.70955891e-04  2.79774697e-01 -8.88353083e-02\n",
      "  2.37845377e-01]\n",
      "kid: [-0.06668165  0.08972566 -0.11288273 -0.03213694 -0.01996813]\n",
      "big: [ 0.02049489  0.06107098 -0.01281123 -0.04079905 -0.01684504]\n",
      "pick: [ 0.14021226  0.11219466 -0.02114077  0.19588986  0.0632673 ]\n",
      "soon: [ 0.20448831 -0.39435108 -0.64473148  0.01340299 -0.15467205]\n",
      "coffee: [ 0.01650865 -0.07738149 -0.22091133  0.0817644  -0.01485897]\n",
      "recommendation: [ 0.14880396  0.15887051  0.10530181  0.03986618 -0.01454931]\n",
      "ground: [ 0.00444902  0.06616211  0.09233331 -0.0977379  -0.00430867]\n",
      "child: [-0.02086532  0.01537468  0.06424978  0.00637407 -0.03126026]\n",
      "sound: [-0.50396917 -0.38282752  0.22546679 -0.07241472  0.02906594]\n",
      "grow: [ 0.00273331 -0.00656964  0.05574982 -0.01587764  0.08455155]\n",
      "system: [-0.06714305  0.23922723 -0.13987848  0.01805839  0.00272839]\n",
      "mysterious: [0.08673881 0.07968871 0.1011611  0.08193467 0.01691025]\n",
      "intense: [-0.17073537  0.00174676  0.00804845  0.00771604  0.03363437]\n",
      "◊û◊ì◊ë◊®◊ô◊ù: [ 0.04229824  0.13255654 -0.06266666 -0.06308821 -0.05520616]\n",
      "hear: [-0.01022312 -0.55623517  0.06448365 -0.17446184 -0.02073639]\n",
      "stumble: [ 0.04131699  0.07505271  0.13330342  0.08258043 -0.04379297]\n",
      "edit: [ 0.13341644  0.00485341 -0.16813719  0.08609996 -0.05514985]\n",
      "exist: [-0.10617669  0.03563799 -0.0345859   0.04794683 -0.07000394]\n",
      "let: [ 0.27112341 -0.40880796 -0.28714887  0.16190723 -0.22682281]\n",
      "course: [-0.3037151  -0.1532314  -0.07480042 -0.04976904 -0.32666832]\n",
      "things: [-0.0125262   0.19494895 -0.02816661 -0.08934348  0.01071654]\n",
      "forward: [0.24223549 0.11266723 0.12977788 0.05706451 0.07727551]\n",
      "character: [-0.0778207   0.0530603  -0.16363078 -0.02588818 -0.13033591]\n",
      "balcony: [0.08955199 0.01738317 0.05221051 0.00528071 0.11015642]\n",
      "suspense: [-0.15737253  0.06552979 -0.08920121 -0.08477832  0.04606876]\n",
      "change: [-0.03201601  0.01102207  0.0715102   0.0923512  -0.08051139]\n",
      "lately: [ 0.1769262   0.11667881  0.04923861  0.16689007 -0.18345734]\n",
      "early: [-0.03183734  0.00724129 -0.11120857 -0.11394639 -0.05886523]\n",
      "believe: [ 0.09454416 -0.0003377  -0.14564596  0.08806116  0.11483454]\n",
      "want: [ 0.08255933 -0.04317569 -0.05103119 -0.09542404  0.24063085]\n",
      "gripping: [ 0.03430564  0.18489964 -0.01342419 -0.12631535  0.0022731 ]\n",
      "deep: [-0.06197349  0.08825897 -0.04665878 -0.00812742  0.03637333]\n",
      "use: [-0.20487871  0.11813085 -0.06975469 -0.00452685 -0.26211057]\n",
      "exercise: [ 0.07752594 -0.01037918  0.09223617  0.06046107 -0.01744006]\n",
      "touch: [-0.20126344 -0.53810097 -0.15183497  0.37404739 -0.01847455]\n",
      "catch: [ 0.17890219 -0.47332763 -0.57287818 -0.0805075  -0.4283429 ]\n",
      "deck: [-0.09007167  0.08476339  0.00138863 -0.07217219  0.03753519]\n",
      "noah: [ 0.00240248  0.20533723 -0.08274372 -0.05360445  0.02840718]\n",
      "rosemary: [ 0.03289708  0.03753479  0.16802685  0.03401357 -0.00692867]\n",
      "conversation: [ 1.21755651e-02 -4.39599946e-05  1.72433683e-02  1.64824259e-01\n",
      " -8.94983024e-02]\n",
      "think: [-0.22848433 -0.13665235  0.13317836  0.22000951 -0.06076815]\n",
      "emma: [ 0.0024937   0.18133071  0.05444654  0.21169516 -0.03335938]\n",
      "look: [ 0.20306864 -0.02034547  0.21506564  0.02672957  0.06807317]\n",
      "testing: [-0.12314421  0.07820026  0.13036794  0.01602745 -0.05290432]\n",
      "difference: [ 0.09843179 -0.04204335  0.06917232 -0.05477794  0.09084569]\n",
      "read: [ 0.13336785  0.23322246  0.05418701 -0.04164764  0.04491484]\n",
      "fresh: [ 0.12115731 -0.13588247  0.07435419 -0.12562965  0.09938689]\n",
      "weekend: [-0.26780844 -0.12115401  0.0280711   0.00413409  0.00384734]\n",
      "paragraph: [-0.00530323  0.11778249 -0.00431643  0.01052895 -0.03923188]\n",
      "sleep: [ 0.03203473 -0.09370216 -0.01145384  0.01181487 -0.01435527]\n",
      "fascinating: [-0.04725977 -0.07239882 -0.23760902 -0.0491184   0.17305316]\n",
      "article: [-0.03164793  0.18002597 -0.03248283 -0.04158542 -0.03478109]\n",
      "late: [-0.19100053  0.18189745  0.25180539  0.2992504  -0.16332913]\n",
      "project: [-0.23642098  0.11065114 -0.30401214  0.31107108  0.07894271]\n",
      "amazing: [-0.32234942 -0.11432075 -0.20134393 -0.19441208  0.05511626]\n",
      "hit: [-0.00541226 -0.1528046   0.05836207 -0.03134878  0.12800138]\n",
      "preppe: [ 0.10115373 -0.14105956 -0.01795068 -0.04845908  0.13365726]\n",
      "game: [-0.34934918 -0.24209371 -0.14051282  0.19304519 -0.18435307]\n",
      "highly: [ 0.02416868  0.12062219 -0.01439931  0.09341091 -0.02989476]\n",
      "afeka: [ 0.15041117  0.14451022 -0.12368757  0.08730713  0.01418996]\n",
      "friend: [-0.06789238 -0.08615226  0.11865121  0.05868139 -0.02472859]\n",
      "history: [-0.07193146  0.1030719  -0.09195396  0.01547728  0.03961844]\n",
      "mean: [ 0.05425403  0.04281323 -0.02029433  0.07528162  0.05112552]\n",
      "travel: [ 0.02236509  0.29208348 -0.04281127  0.09657203 -0.02379431]\n",
      "book: [0.1913698  0.08721388 0.03036221 0.12365073 0.05801305]\n",
      "easy: [ 0.1217066   0.07626236 -0.08245816  0.14692001 -0.01058884]\n",
      "office: [-0.0827651   0.02053899 -0.07709667  0.1453252   0.07149455]\n",
      "harari: [-0.0090013   0.17208939 -0.07266423 -0.02684022  0.03669986]\n",
      "bad: [-0.05453356  0.03564413 -0.19289975 -0.01364477  0.02423704]\n",
      "actually: [ 0.01292414  0.09761627 -0.07646473 -0.06046567 -0.16861318]\n",
      "gpt: [ 0.19037761  0.03005854  0.12192445  0.10519942 -0.05814267]\n",
      "hand: [-0.02170551 -0.02649766  0.13660688  0.08937915  0.0928375 ]\n",
      "night: [-0.4484943   0.06446813  0.09411436  0.20740932 -0.29897683]\n",
      "surprisingly: [ 0.11865555  0.18436812  0.0870108  -0.03361657 -0.000384  ]\n",
      "noam: [ 0.07288601  0.10515884 -0.13087434 -0.02595968 -0.36096595]\n",
      "tip: [ 0.12170447 -0.07080254  0.05043895 -0.01348997  0.09835911]\n",
      "hobby: [ 0.10070675 -0.01551513 -0.08700127  0.26782677  0.09290919]\n",
      "know: [-0.20826695 -0.05235742  0.3035392   0.08689686 -0.10951414]\n",
      "enhance: [ 0.05341322 -0.09180776 -0.17304831  0.00555498 -0.07403125]\n",
      "talk: [ 0.26477131 -0.05426106 -0.31188471 -0.04061786 -0.30364132]\n",
      "provoking: [ 0.03070939  0.02717005  0.13520168  0.07636999 -0.02364601]\n",
      "currently: [ 0.1407622   0.05318066 -0.21243716 -0.08694525 -0.00873435]\n",
      "little: [0.32206579 0.08841551 0.218968   0.03595203 0.11232056]\n",
      "exit: [-0.06650404 -0.15705651 -0.0030147   0.07494402  0.05764714]\n",
      "problem: [-0.11462728 -0.04106161 -0.14199965  0.00808598 -0.02741059]\n",
      "couple: [ 0.09310688  0.01021125 -0.15045709  0.10204766 -0.0003738 ]\n",
      "school: [ 0.01449286  0.02065317 -0.10654698  0.28968993  0.0429678 ]\n",
      "detective: [ 0.08178835  0.16260912  0.01941819 -0.07394847  0.01071644]\n",
      "speak: [-0.21587012  0.12684849 -0.2751234  -0.20290553  0.07683456]\n",
      "language: [ 0.13502657  0.16743123 -0.34347119  0.02737908  0.18703408]\n",
      "tell: [ 0.08455375 -0.16764561 -0.03773214 -0.00850801  0.04288905]\n",
      "write: [ 0.03763806 -0.01540351  0.0793158  -0.11477124  0.0011339 ]\n",
      "awesome: [ 0.14296637 -0.27807375  0.13047451 -0.15068648  0.08179919]\n",
      "like: [-0.16972015 -0.34444104  0.12933079 -0.05722685 -0.01643142]\n",
      "learn: [ 0.26194925  0.10493864 -0.46718868 -0.07154472  0.18243306]\n",
      "definitely: [-0.10331909 -0.62831413 -0.24740803 -0.13102538 -0.39572558]\n",
      "humankind: [-0.06531388  0.07090382 -0.03428282  0.05034282  0.0216357 ]\n",
      "software: [-0.12247573  0.1117656  -0.21882501  0.14276175  0.0459277 ]\n",
      "mint: [0.0119639  0.10797871 0.02539376 0.0187216  0.02508131]\n",
      "old: [-0.11421362 -0.16014763  0.0011548  -0.09845764  0.01824401]\n",
      "gardening: [ 0.07499791  0.06276608 -0.01523843 -0.02055796  0.10385882]\n",
      "storyline: [ 0.04624151  0.19475382 -0.11635545 -0.02067288 -0.18182496]\n",
      "try: [ 0.19931058 -0.07438391 -0.04218037  0.07513189  0.26776529]\n",
      "yuval: [-0.01754363  0.16181882 -0.0978415  -0.1064951   0.04757631]\n",
      "nice: [0.3088499  0.02469653 0.31472793 0.08352988 0.08118915]\n",
      "sign: [-0.10435279  0.06046207  0.01867055 -0.11010423 -0.05789161]\n",
      "role: [ 6.33817712e-03  5.45463953e-02  9.31515815e-02 -6.39112914e-05\n",
      " -1.20413846e-01]\n",
      "great: [-0.15490318 -0.81927931  0.089379   -0.45177813 -0.15151155]\n",
      "miss: [ 0.17143345  0.0727071  -0.18827225 -0.08184947 -0.22121867]\n",
      "therapeutic: [ 0.10505538  0.16565575  0.02654192 -0.0446105   0.02836764]\n",
      "sun: [-0.00536145  0.06356083  0.02803007 -0.02112717  0.01361945]\n",
      "generate: [ 0.0446088   0.00118464  0.11675291  0.16582394 -0.07669778]\n",
      "development: [-0.14916111  0.05102287 -0.19866867  0.12500111  0.04856472]\n",
      "watch: [-0.03691864  0.13440429 -0.11585891 -0.252025   -0.16966083]\n",
      "recipe: [0.17905809 0.01097272 0.01243924 0.18641157 0.13251918]\n",
      "tough: [-0.24079465 -0.07528318  0.01004354 -0.03931765  0.03735903]\n",
      "thing: [-0.07738833  0.06675899 -0.04167389 -0.29582463 -0.03047719]\n",
      "natural: [ 0.21532237  0.12590367 -0.25977209  0.0700127   0.1802215 ]\n",
      "sapiens: [-0.03198101  0.08462939 -0.04545695  0.01536788  0.04487043]\n",
      "message: [ 0.20370903  0.05291037 -0.18137021  0.17089498 -0.13254366]\n",
      "busy: [ 0.098376    0.13401016 -0.15544688  0.18769879 -0.18403295]\n",
      "nostalgia: [-0.34751723  0.07667527 -0.29183385 -0.20504728  0.06840862]\n",
      "challenge: [ 0.15202422  0.17625434  0.16111082  0.0653633  -0.07239554]\n",
      "fantastic: [ 0.07945399  0.01102663  0.06477838 -0.03801283  0.10113826]\n",
      "backend: [-0.02567819  0.29865178 -0.04892807  0.12382475 -0.04064045]\n",
      "chat: [-0.03093687  0.01852633  0.06678238 -0.02657228  0.01908288]\n",
      "today: [ 0.05390114 -0.03402191  0.01134619 -0.13588174 -0.32777268]\n",
      "dive: [-0.11217168  0.04070716 -0.0916819  -0.00846725  0.05411649]\n",
      "anytime: [ 0.04375243  0.0107618  -0.04669089  0.14778637  0.05833483]\n",
      "debug: [-0.19354104  0.03835664  0.11478363  0.14298404 -0.07552377]\n",
      "fun: [ 0.04165418  0.12201238  0.13189209  0.01944577 -0.08572294]\n",
      "nightmare: [-0.04991874  0.08526768 -0.15696207 -0.03278283  0.02306728]\n",
      "new: [-0.02324719 -0.47321669 -0.19223809  1.02709904  0.26218164]\n",
      "sure: [ 0.05178292 -0.12014832  0.05348642  0.02956434 -0.20070642]\n",
      "shine: [ 0.00940094  0.13327776 -0.02953383 -0.141153    0.02339331]\n",
      "cool: [0.05899638 0.06616074 0.08013877 0.04930387 0.00361876]\n",
      "wars: [-0.02047557 -0.14792435  0.02174653  0.0234476   0.03436277]\n",
      "love: [-0.14407556 -0.07239705 -0.07204224  0.04692708 -0.02115847]\n",
      "idea: [ 0.20593066 -0.07341236  0.04186113 -0.0856512  -0.39180969]\n",
      "swamp: [ 0.07770818  0.02047822 -0.01646492  0.39519532  0.04210135]\n",
      "beautiful: [ 0.0179117   0.09093933 -0.01733596 -0.14573752 -0.03187484]\n",
      "sam: [-0.11466661 -0.11398787 -0.15651818  0.1073375   0.07303409]\n",
      "cream: [ 0.0068483   0.05485005  0.04145814  0.00446088 -0.02526419]\n",
      "skill: [ 0.10014742 -0.19993904  0.05462079  0.39868679  0.15715306]\n",
      "balance: [-0.08370322  0.0238687   0.00336769 -0.02169615  0.02216873]\n",
      "maybe: [ 0.02961486 -0.25704242  0.02076792 -0.00518972 -0.04275965]\n",
      "got: [0.06515007 0.03155003 0.07962341 0.02717222 0.07653037]\n",
      "relax: [ 0.19204493  0.12998657 -0.0711505  -0.04235676 -0.32471282]\n",
      "kind: [-0.12027735  0.0537901  -0.04592173  0.07301544  0.02189702]\n",
      "bump: [ 0.08879564  0.02761853 -0.06393622 -0.20206372 -0.06407731]\n",
      "squeeze: [-0.02421314 -0.01732678 -0.09873975 -0.10532891  0.06710259]\n",
      "listen: [ 0.01642922  0.09050204  0.01537275 -0.09209797  0.02708201]\n",
      "mandalorian: [ 0.09010548  0.22877298 -0.06260614 -0.09966986 -0.20519624]\n",
      "finally: [ 0.11235748 -0.03245144  0.1411004   0.02679086 -0.08148977]\n",
      "work: [-0.22346742 -0.24113449 -0.43613337  0.79336658  0.0049537 ]\n",
      "start: [ 0.22138244 -0.12487157 -0.22155452 -0.10359153  0.23081313]\n",
      "analysis: [ 0.12150986  0.03721695 -0.03400993 -0.01226443  0.05621622]\n",
      "day: [ 0.06371785  0.10535746  0.09872556 -0.02053715 -0.05523798]\n",
      "cooking: [ 0.39405879 -0.08781648  0.27042041 -0.00262026  0.26390979]\n",
      "place: [-0.18056304 -0.06650358  0.20066498  0.12366884 -0.08833466]\n",
      "perfect: [-0.10959719 -0.12923934  0.08535946 -0.0516923  -0.01332091]\n",
      "gang: [-0.14713988 -0.16200391 -0.12684486 -0.11472633  0.03858606]\n",
      "photography: [-0.09059138  0.21486566 -0.09439873  0.02350794 -0.0113152 ]\n",
      "beach: [-0.06188886  0.0030436   0.05524539 -0.07877815  0.00519374]\n",
      "bit: [ 0.28565351 -0.0549139   0.18854521  0.08240186  0.04588757]\n",
      "rewarding: [-0.16481592  0.09370039 -0.03266169 -0.06953584  0.01437945]\n",
      "recommend: [-0.1257948   0.00353119 -0.18124423 -0.03172431  0.04661462]\n",
      "brightly: [ 0.00594618  0.08621528  0.04457437 -0.05832943  0.01615906]\n",
      "haha: [-0.04094012  0.06875975 -0.0663197  -0.00268475 -0.02762133]\n",
      "come: [-0.17020087  0.01842037  0.04892831 -0.03074106  0.00376053]\n",
      "◊ë◊ê◊†◊í◊ú◊ô◊™: [ 0.09756519  0.15372278  0.01636651  0.01193263 -0.12431356]\n",
      "care: [ 0.2338746  -0.12812    -0.33875007 -0.0817965  -0.14240331]\n",
      "job: [ 0.08641705  0.18060056 -0.07288226  0.16708063  0.04203084]\n",
      "plan: [-0.29432904 -0.41877723 -0.13882648 -0.12447758 -0.26280412]\n",
      "struggle: [0.10617535 0.16131869 0.03858638 0.00636106 0.03275635]\n",
      "create: [ 0.09343824  0.12126367  0.10189838  0.01273259 -0.00355021]\n",
      "hey: [ 0.26568416  0.2093766   0.00980453 -0.07169219 -0.63294238]\n",
      "hope: [ 0.10308092  0.12330036 -0.19056394 -0.16009446 -0.27040568]\n",
      "meet: [-0.01109762 -0.03259962 -0.16092741  0.04959889  0.00283134]\n",
      "list: [-0.02248247 -0.0360425  -0.02596147 -0.25700007 -0.05457684]\n",
      "online: [-0.13939227  0.08286888 -0.12002044 -0.07410501 -0.01712185]\n",
      "bunch: [-0.02615593  0.01766548 -0.13442266  0.27757488  0.063764  ]\n",
      "excited: [-0.10445515  0.04656006 -0.05678341  0.05436289  0.02843974]\n",
      "end: [ 0.01519486  0.12500651  0.02674733 -0.0946663   0.01294869]\n",
      "far: [-0.04980514  0.0344773  -0.07320921 -0.05609488  0.09554651]\n",
      "finish: [ 0.03312163  0.09805346  0.08599031  0.00699004 -0.09227043]\n",
      "curious: [-0.00121534  0.17903266  0.05573524 -0.09471494 -0.06287959]\n",
      "good: [ 0.32451751  0.18579287 -0.28955984 -0.07989374 -1.20787958]\n",
      "thank: [ 0.10073955 -0.09453131 -0.08426477 -0.23486392 -0.29762759]\n",
      "lot: [-0.20691961  0.30783831 -0.05151277  0.25076968 -0.170338  ]\n",
      "processing: [ 0.17263081  0.16129887 -0.32126712  0.06702308  0.22382606]\n",
      "interesting: [ 0.07805558  0.04230307 -0.13233869  0.30508764  0.09664524]\n",
      "board: [-0.06627705 -0.03231123 -0.07758894  0.17979458  0.01734385]\n",
      "garden: [0.222666   0.03195737 0.06332554 0.05007588 0.15771424]\n",
      "upgrade: [-0.09660544  0.16144489 -0.16416076  0.07388355  0.03739868]\n",
      "basil: [-0.0129331   0.0838029  -0.00045371 -0.02830112  0.04198408]\n",
      "solve: [ 0.05920439  0.12354047  0.04409291 -0.01828546  0.00957384]\n",
      "case: [ 0.05547093  0.14355408 -0.0775868  -0.04399559  0.04526951]\n",
      "yeah: [ 0.02111228  0.04824659 -0.02976407 -0.06815849 -0.04795584]\n",
      "ice: [-0.011675    0.01390931  0.05142101  0.02243363 -0.02293381]\n",
      "star: [-0.00595593 -0.07558619  0.06256182  0.06884911  0.01958575]\n",
      "check: [-0.07214511 -0.24079262 -0.04238814 -0.22649569  0.02565063]\n",
      "forget: [ 0.1260319  -0.0882316   0.09326315 -0.01295369  0.06415832]\n",
      "add: [ 0.00191492  0.02686135 -0.04351219 -0.14422307 -0.02773284]\n",
      "stranger: [-0.01005746  0.23019206 -0.09076417 -0.1572867   0.01413591]\n",
      "group: [-0.02331733  0.09212258 -0.11108004 -0.1374589   0.03452818]\n",
      "encrypt: [-0.02543313  0.02602702  0.06074345 -0.01413509  0.01632505]\n",
      "stop: [-0.06715575  0.12344506 -0.09542246 -0.1745523   0.02864548]\n",
      "warm: [ 0.00105911  0.11166604 -0.00269084 -0.10025528  0.02651076]\n",
      "right: [-0.10969473  0.16230829 -0.11796871 -0.12893196  0.03563929]\n",
      "text: [ 0.14703832  0.35543454  0.0193334   0.12858784 -0.33447186]\n",
      "way: [-0.18098458 -0.47217501 -0.10571642 -0.08563684  0.23832473]\n",
      "remember: [-0.20588051  0.06290539 -0.0416642  -0.03730077 -0.04737804]\n",
      "drag: [-0.04189233  0.20706677  0.0427418   0.02500843 -0.05951895]\n",
      "requirement: [ 0.19154481  0.21064121 -0.07496815 -0.00401162  0.03829481]\n",
      "rec: [ 0.06766063 -0.02223496 -0.054164   -0.15754753  0.01708912]\n",
      "whatsapp: [ 0.04308537  0.10259789  0.07012411 -0.07098765  0.01328882]\n",
      "clever: [ 0.07807963  0.12229239  0.08937142  0.00082777 -0.00869121]\n",
      "eat: [ 0.16585878  0.10025798  0.00380349 -0.08171743 -0.24364172]\n",
      "ask: [ 0.16023838  0.02845581 -0.00689913  0.13311748  0.14335191]\n",
      "hangover: [ 0.04752061  0.02227144 -0.02833134  0.08083866 -0.12700462]\n",
      "super: [-0.00695701  0.08930907 -0.10864289  0.142678    0.06458625]\n",
      "oh: [ 0.20169826 -0.25934922 -0.00560721 -0.06778994  0.10205352]\n"
     ]
    }
   ],
   "source": [
    "for word, idx in word_to_id.items():\n",
    "    print(f\"{word}: {word_vectors[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanation of applying GloVe:**\n",
    "*Loss Over Iterations:*\n",
    "The loss values decrease over iterations, indicating that the model is learning and optimizing the word vectors effectively.\n",
    "The loss decreases significantly in the early iterations and continues to decrease at a slower rate as training progresses. This indicates that the model is converging towards an optimal set of word vectors.\n",
    "\n",
    "*Word Vectors:*\n",
    "The final word vectors are the main output of the GloVe algorithm. Each word in your dataset is represented by a vector in a multi-dimensional space.\n",
    "Each vector captures the semantic properties of the corresponding word. Words with similar meanings or usage patterns tend to have similar vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying tagging by CYK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to apply the CYK tagging to 5 sentences, 1 manually and 4 with code.\n",
    "The manual cyk tagging is in the attached word file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyk_parse(sentence, grammar):\n",
    "    # Step 1: Tokenization\n",
    "    tokens = sentence.split()\n",
    "    n = len(tokens)\n",
    "    table = [[set() for _ in range(n+1)] for _ in range(n+1)]\n",
    "\t\n",
    "    # Step 2: Initialization\n",
    "    for i in range(1, n+1):\n",
    "        for rule in grammar:\n",
    "            if rule[1] == tokens[i-1]:\n",
    "                table[i][i].add(rule[0])\n",
    "\n",
    "    # Step 3: Rule Application\n",
    "    for length in range(2, n+1):\n",
    "        for i in range(1, n-length+2):\n",
    "            j = i + length - 1\n",
    "            for k in range(i, j):\n",
    "                for rule in grammar:\n",
    "                    if len(rule) == 3:\n",
    "                        for left in table[i][k]:\n",
    "                            for right in table[k+1][j]:\n",
    "                                if rule[1] in left and rule[2] in right:\n",
    "                                    table[i][j].add(rule[0])\n",
    "\n",
    "\n",
    "    # Step 4: Backtracking\n",
    "    if 'S' in table[1][n]:\n",
    "        return True, table\n",
    "    else:\n",
    "        return False, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences that will be parsed\n",
    "sentence_1 = whatsapp_df_copy['whatsapp_text'][19] # early today was a beautiful day\n",
    "sentence_2 = whatsapp_df_copy['whatsapp_text'][20] # the sun shone brightly warming the ground\n",
    "sentence_3 = whatsapp_df_copy['whatsapp_text'][21] # I went to the beach with a friend\n",
    "sentence_4 = whatsapp_df_copy['whatsapp_text'][22] # the children ate ice-cream\n",
    "\n",
    "sentences = [sentence_1, sentence_2, sentence_3, sentence_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context-free grammar in CNF\n",
    "grammar_1 = [\n",
    "    ('S', 'NP', 'VP'),\n",
    "    ('NP', 'RB', 'NN'),\n",
    "    ('NP', 'DT', 'NPA'),\n",
    "    ('NPA', 'JJ', 'NN'),\n",
    "    ('RB', 'early'),\n",
    "    ('NN', 'today'),\n",
    "    ('VP', 'VBD', 'NP'),\n",
    "    ('VBD', 'was'),\n",
    "    ('DT', 'a'),\n",
    "    ('JJ', 'beautiful'),\n",
    "    ('NN', 'day')\n",
    "]\n",
    "\n",
    "grammar_2 = [\n",
    "    ('S', 'NP', 'VP'),\n",
    "    ('NP', 'DT', 'NN'),\n",
    "    ('DT', 'the'),\n",
    "    ('NN', 'sun'),\n",
    "    ('VP', 'VBD', 'S'),\n",
    "    ('VBD', 'shone'),\n",
    "    ('S', 'RB', 'VP'),\n",
    "    ('RB', 'brightly'),\n",
    "    ('VP', 'VBG', 'NP'),\n",
    "    ('VBG', 'warming'),\n",
    "    ('DT', 'the'),\n",
    "    ('NN', 'ground')\n",
    "]\n",
    "\n",
    "grammar_3 = [\n",
    "    ('S', 'NP', 'VP'),\n",
    "    ('NP', 'I'),\n",
    "    ('VP', 'VBD', 'VP_0'),\n",
    "    ('VP_0', 'PP', 'PP'),\n",
    "    ('VBD', 'went'),\n",
    "    ('PP', 'IN', 'NP'),\n",
    "    ('IN', 'to'),\n",
    "    ('IN', 'with'),\n",
    "    ('NP', 'DT', 'NN'),\n",
    "    ('DT', 'the'),\n",
    "    ('DT', 'a'),\n",
    "    ('NN', 'beach'),\n",
    "    ('NN', 'friend')\n",
    "]\n",
    "\n",
    "grammar_4 = [\n",
    "    ('S', 'NP', 'VP'),\n",
    "    ('NP', 'DT', 'NN'),\n",
    "    ('NN', 'children'),\n",
    "    ('DT', 'the'),\n",
    "    ('VP', 'VBD', 'NN'),\n",
    "    ('VBD', 'ate'),\n",
    "    ('NN', 'ice-cream')\n",
    "]\n",
    "\n",
    "grammars = [grammar_1, grammar_2, grammar_3, grammar_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:   early today was a beautiful day\n",
      "Parse table: \n",
      "[set(), set(), set(), set(), set(), set(), set()]\n",
      "[set(), {'RB'}, {'NP'}, set(), set(), set(), {'S'}]\n",
      "[set(), set(), {'NN'}, set(), set(), set(), set()]\n",
      "[set(), set(), set(), {'VBD'}, set(), set(), {'VP'}]\n",
      "[set(), set(), set(), set(), {'DT'}, set(), {'NP'}]\n",
      "[set(), set(), set(), set(), set(), {'JJ'}, {'NPA'}]\n",
      "[set(), set(), set(), set(), set(), set(), {'NN'}]\n",
      "\n",
      "\n",
      "Input sentence:   the sun shone brightly warming the ground\n",
      "Parse table: \n",
      "[set(), set(), set(), set(), set(), set(), set(), set()]\n",
      "[set(), {'DT'}, {'NP'}, set(), set(), set(), set(), {'S'}]\n",
      "[set(), set(), {'NN'}, set(), set(), set(), set(), set()]\n",
      "[set(), set(), set(), {'VBD'}, set(), set(), set(), {'VP'}]\n",
      "[set(), set(), set(), set(), {'RB'}, set(), set(), {'S'}]\n",
      "[set(), set(), set(), set(), set(), {'VBG'}, set(), {'VP'}]\n",
      "[set(), set(), set(), set(), set(), set(), {'DT'}, {'NP'}]\n",
      "[set(), set(), set(), set(), set(), set(), set(), {'NN'}]\n",
      "\n",
      "\n",
      "Input sentence:   I went to the beach with a friend\n",
      "Parse table: \n",
      "[set(), set(), set(), set(), set(), set(), set(), set(), set()]\n",
      "[set(), {'NP'}, set(), set(), set(), set(), set(), set(), {'S'}]\n",
      "[set(), set(), {'VBD'}, set(), set(), set(), set(), set(), {'VP'}]\n",
      "[set(), set(), set(), {'IN'}, set(), {'PP'}, set(), set(), {'VP_0'}]\n",
      "[set(), set(), set(), set(), {'DT'}, {'NP'}, set(), set(), set()]\n",
      "[set(), set(), set(), set(), set(), {'NN'}, set(), set(), set()]\n",
      "[set(), set(), set(), set(), set(), set(), {'IN'}, set(), {'PP'}]\n",
      "[set(), set(), set(), set(), set(), set(), set(), {'DT'}, {'NP'}]\n",
      "[set(), set(), set(), set(), set(), set(), set(), set(), {'NN'}]\n",
      "\n",
      "\n",
      "Input sentence:   the children ate ice-cream\n",
      "Parse table: \n",
      "[set(), set(), set(), set(), set()]\n",
      "[set(), {'DT'}, {'NP'}, set(), {'S'}]\n",
      "[set(), set(), {'NN'}, set(), set()]\n",
      "[set(), set(), set(), {'VBD'}, {'VP'}]\n",
      "[set(), set(), set(), set(), {'NN'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the CYK parser for each sentence\n",
    "for sentence, grammar in zip(sentences, grammars):\n",
    "    parsed, table = cyk_parse(sentence, grammar)\n",
    "\n",
    "    # Print the parse table and whether the sentence was parsed or not\n",
    "    if parsed:\n",
    "        print(\"Input sentence: \", sentence)\n",
    "        print(\"Parse table: \")\n",
    "        for row in table:\n",
    "            print(row)\n",
    "    else:\n",
    "        print(\"Input sentence: \", sentence)\n",
    "        print(\"Sentence not parsed.\")\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
